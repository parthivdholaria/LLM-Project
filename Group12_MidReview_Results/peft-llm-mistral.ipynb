{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-20T10:49:42.989137Z","iopub.execute_input":"2024-10-20T10:49:42.989513Z","iopub.status.idle":"2024-10-20T10:49:43.515755Z","shell.execute_reply.started":"2024-10-20T10:49:42.989478Z","shell.execute_reply":"2024-10-20T10:49:43.514782Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"! pip install transformers datasets peft torch\n! pip install -U bitsandbytes\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T10:49:43.517851Z","iopub.execute_input":"2024-10-20T10:49:43.518390Z","iopub.status.idle":"2024-10-20T10:50:12.817960Z","shell.execute_reply.started":"2024-10-20T10:49:43.518344Z","shell.execute_reply":"2024-10-20T10:50:12.816962Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\nfrom datasets import load_dataset\nfrom peft import PromptTuningConfig, get_peft_model\n\n# Load the model and tokenizer with 4-bit quantization enabled and CPU offloading\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    use_auth_token=\"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\",\n    load_in_4bit=True,  # Enable 4-bit quantization\n    device_map=\"cuda\"  # Automatically map layers to available devices (CPU & GPU)  # Offload some layers to CPU in 32-bit precision\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_auth_token=\"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\"\n)\n\n# Define the Prompt Tuning configuration\npeft_config = PromptTuningConfig(\n    task_type=\"CAUSAL_LM\",    # Set the task type for causal language modeling\n    num_virtual_tokens=20     # Number of virtual tokens to tune (instead of prompt_length)\n)\n\n# Apply prompt tuning to the model\npeft_model = get_peft_model(model, peft_config)\n\n# Load the SVAMP dataset\ndataset = load_dataset(\"ChilleD/SVAMP\", split='train')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T10:50:12.819413Z","iopub.execute_input":"2024-10-20T10:50:12.819731Z","iopub.status.idle":"2024-10-20T10:51:40.244786Z","shell.execute_reply.started":"2024-10-20T10:50:12.819697Z","shell.execute_reply":"2024-10-20T10:51:40.242143Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80c6c993bcec427c912405f226b0021c"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548a70624fc744ca9a3388091efd4f62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f113416926154e8d94690c19bba3a7b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21eac2858f704705a74bcf9060ac5394"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0ed8d77279e4dc3b14c50e3f73b4203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768173229afd4a1d8396b9184bc0f625"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9a1bdcf851f489598a245b44b6608c3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36605bfbe48f46f9a2ed3f952bc83ed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1abb4ffae53545d889914d9f8990c10e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38bb6bb3a9104cc1b9fcc05f9016b0c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e9117fd7ec4b76a6b1be0e94919d5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/675 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c0030d488349c9b8444349e094c411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/111k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a913f20edde4623a9b01402464370b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/54.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f496b50a2b5402eb9e9f97d8e2a28c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ae1040045b4d2998b050389f6f8330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a87991c6c7549ebad3d81cd040957e8"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess the dataset\n# Set the maximum length for tokenization\nMAX_LENGTH = 512  # Adjust this depending on the model's input size limit (common is 512 for many models)\n\n# Preprocess the dataset with truncation and padding to MAX_LENGTH\n# Set the maximum length for tokenization\nMAX_LENGTH = 512  # Adjust this depending on the model's input size limit\n\n# Set pad_token to eos_token if pad_token doesn't exist\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Preprocess function as defined previously\ndef preprocess_function(example):\n    inputs = example['Body'] + example['Question']\n    targets = example['Answer']\n    \n    # Tokenize the inputs and targets with max_length\n    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n    labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n    \n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Prepare the list to hold preprocessed data\npreprocessed_data = []\n\n# Loop through each sample in the dataset\nfor example in dataset:\n    # Apply the preprocess function to each sample\n    preprocessed_example = preprocess_function(example)\n    preprocessed_data.append(preprocessed_example)\n\n# Convert the list of preprocessed data to the required format for training\n\nprint(len(preprocessed_data))\nfrom datasets import Dataset\ntrain_dataset = Dataset.from_dict({key: [example[key] for example in preprocessed_data] for key in preprocessed_data[0].keys()})\n\n# Now `train_dataset` can be used in training\n\n\n\n# Define the optimizer and scheduler\noptimizer = AdamW(peft_model.parameters(), lr=5e-5)\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataset)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Move model to GPU if available, CPU offloading is handled by `device_map`\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\npeft_model.to(device)\n\n# Custom training loop\nfor epoch in range(num_epochs):\n    peft_model.train()  # Set the model to training mode\n    total_loss = 0\n    \n    for idx in range(100):\n        batch = train_dataset[idx]\n#         print(\"Step: \", step)\n#         print(\"\\n\")\n#         print(\"Batch: \", batch)\n        # Move inputs and labels to device\n        input_ids = torch.tensor(batch['input_ids']).unsqueeze(0).to(device)\n        labels = torch.tensor(batch['labels']).unsqueeze(0).to(device)\n\n        # Forward pass\n        outputs = peft_model(input_ids=input_ids, labels=labels)\n        loss = outputs.loss\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()  # Update learning rate\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n        # Print progress\n        print(f\"Epoch: {epoch + 1}/{num_epochs}, Step: {idx}, Loss: {loss.item()}\")\n\n    avg_loss = total_loss / len(train_dataset)\n    print(f\"Epoch {epoch + 1} finished. Average Loss: {avg_loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T10:52:50.388433Z","iopub.execute_input":"2024-10-20T10:52:50.388863Z","iopub.status.idle":"2024-10-20T11:08:56.566702Z","shell.execute_reply.started":"2024-10-20T10:52:50.388826Z","shell.execute_reply":"2024-10-20T11:08:56.565662Z"}},"outputs":[{"name":"stdout","text":"700\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/3, Step: 0, Loss: 8.79361629486084\nEpoch: 1/3, Step: 1, Loss: 8.808880805969238\nEpoch: 1/3, Step: 2, Loss: 8.802851676940918\nEpoch: 1/3, Step: 3, Loss: 8.71268367767334\nEpoch: 1/3, Step: 4, Loss: 8.763748168945312\nEpoch: 1/3, Step: 5, Loss: 8.72416877746582\nEpoch: 1/3, Step: 6, Loss: 8.674636840820312\nEpoch: 1/3, Step: 7, Loss: 8.706826210021973\nEpoch: 1/3, Step: 8, Loss: 8.688374519348145\nEpoch: 1/3, Step: 9, Loss: 8.686456680297852\nEpoch: 1/3, Step: 10, Loss: 8.650683403015137\nEpoch: 1/3, Step: 11, Loss: 8.648435592651367\nEpoch: 1/3, Step: 12, Loss: 8.627275466918945\nEpoch: 1/3, Step: 13, Loss: 8.589165687561035\nEpoch: 1/3, Step: 14, Loss: 8.582572937011719\nEpoch: 1/3, Step: 15, Loss: 8.579200744628906\nEpoch: 1/3, Step: 16, Loss: 8.519889831542969\nEpoch: 1/3, Step: 17, Loss: 8.490193367004395\nEpoch: 1/3, Step: 18, Loss: 8.514581680297852\nEpoch: 1/3, Step: 19, Loss: 8.51151180267334\nEpoch: 1/3, Step: 20, Loss: 8.422450065612793\nEpoch: 1/3, Step: 21, Loss: 8.436660766601562\nEpoch: 1/3, Step: 22, Loss: 8.439555168151855\nEpoch: 1/3, Step: 23, Loss: 8.422870635986328\nEpoch: 1/3, Step: 24, Loss: 8.360554695129395\nEpoch: 1/3, Step: 25, Loss: 8.41041374206543\nEpoch: 1/3, Step: 26, Loss: 8.346915245056152\nEpoch: 1/3, Step: 27, Loss: 8.354774475097656\nEpoch: 1/3, Step: 28, Loss: 8.336479187011719\nEpoch: 1/3, Step: 29, Loss: 8.297158241271973\nEpoch: 1/3, Step: 30, Loss: 8.314285278320312\nEpoch: 1/3, Step: 31, Loss: 8.298699378967285\nEpoch: 1/3, Step: 32, Loss: 8.28168773651123\nEpoch: 1/3, Step: 33, Loss: 8.250035285949707\nEpoch: 1/3, Step: 34, Loss: 8.25986385345459\nEpoch: 1/3, Step: 35, Loss: 8.240605354309082\nEpoch: 1/3, Step: 36, Loss: 8.220714569091797\nEpoch: 1/3, Step: 37, Loss: 8.213632583618164\nEpoch: 1/3, Step: 38, Loss: 8.220368385314941\nEpoch: 1/3, Step: 39, Loss: 8.189678192138672\nEpoch: 1/3, Step: 40, Loss: 8.131458282470703\nEpoch: 1/3, Step: 41, Loss: 8.13453483581543\nEpoch: 1/3, Step: 42, Loss: 8.08693790435791\nEpoch: 1/3, Step: 43, Loss: 8.103215217590332\nEpoch: 1/3, Step: 44, Loss: 8.095354080200195\nEpoch: 1/3, Step: 45, Loss: 8.09235668182373\nEpoch: 1/3, Step: 46, Loss: 8.042661666870117\nEpoch: 1/3, Step: 47, Loss: 8.050536155700684\nEpoch: 1/3, Step: 48, Loss: 8.029295921325684\nEpoch: 1/3, Step: 49, Loss: 7.985192775726318\nEpoch: 1/3, Step: 50, Loss: 8.010110855102539\nEpoch: 1/3, Step: 51, Loss: 8.004589080810547\nEpoch: 1/3, Step: 52, Loss: 7.954747200012207\nEpoch: 1/3, Step: 53, Loss: 7.947769641876221\nEpoch: 1/3, Step: 54, Loss: 7.913949966430664\nEpoch: 1/3, Step: 55, Loss: 7.890163898468018\nEpoch: 1/3, Step: 56, Loss: 7.9128828048706055\nEpoch: 1/3, Step: 57, Loss: 7.915892124176025\nEpoch: 1/3, Step: 58, Loss: 7.913114547729492\nEpoch: 1/3, Step: 59, Loss: 7.816685676574707\nEpoch: 1/3, Step: 60, Loss: 7.854996681213379\nEpoch: 1/3, Step: 61, Loss: 7.790885925292969\nEpoch: 1/3, Step: 62, Loss: 7.797004699707031\nEpoch: 1/3, Step: 63, Loss: 7.812071323394775\nEpoch: 1/3, Step: 64, Loss: 7.817995548248291\nEpoch: 1/3, Step: 65, Loss: 7.790596961975098\nEpoch: 1/3, Step: 66, Loss: 7.713698863983154\nEpoch: 1/3, Step: 67, Loss: 7.728646278381348\nEpoch: 1/3, Step: 68, Loss: 7.710625171661377\nEpoch: 1/3, Step: 69, Loss: 7.721992015838623\nEpoch: 1/3, Step: 70, Loss: 7.694021224975586\nEpoch: 1/3, Step: 71, Loss: 7.675760746002197\nEpoch: 1/3, Step: 72, Loss: 7.657087802886963\nEpoch: 1/3, Step: 73, Loss: 7.654420375823975\nEpoch: 1/3, Step: 74, Loss: 7.622305870056152\nEpoch: 1/3, Step: 75, Loss: 7.623764514923096\nEpoch: 1/3, Step: 76, Loss: 7.580348968505859\nEpoch: 1/3, Step: 77, Loss: 7.542865753173828\nEpoch: 1/3, Step: 78, Loss: 7.557403564453125\nEpoch: 1/3, Step: 79, Loss: 7.543463230133057\nEpoch: 1/3, Step: 80, Loss: 7.513862133026123\nEpoch: 1/3, Step: 81, Loss: 7.505767822265625\nEpoch: 1/3, Step: 82, Loss: 7.5700836181640625\nEpoch: 1/3, Step: 83, Loss: 7.478510856628418\nEpoch: 1/3, Step: 84, Loss: 7.473069190979004\nEpoch: 1/3, Step: 85, Loss: 7.464449405670166\nEpoch: 1/3, Step: 86, Loss: 7.413805961608887\nEpoch: 1/3, Step: 87, Loss: 7.433144569396973\nEpoch: 1/3, Step: 88, Loss: 7.448577880859375\nEpoch: 1/3, Step: 89, Loss: 7.413331031799316\nEpoch: 1/3, Step: 90, Loss: 7.38881778717041\nEpoch: 1/3, Step: 91, Loss: 7.349795341491699\nEpoch: 1/3, Step: 92, Loss: 7.3489484786987305\nEpoch: 1/3, Step: 93, Loss: 7.322249412536621\nEpoch: 1/3, Step: 94, Loss: 7.3063249588012695\nEpoch: 1/3, Step: 95, Loss: 7.331984519958496\nEpoch: 1/3, Step: 96, Loss: 7.323775768280029\nEpoch: 1/3, Step: 97, Loss: 7.287196636199951\nEpoch: 1/3, Step: 98, Loss: 7.24469518661499\nEpoch: 1/3, Step: 99, Loss: 7.250025272369385\nEpoch 1 finished. Average Loss: 1.1445471109662737\nEpoch: 2/3, Step: 0, Loss: 7.230947017669678\nEpoch: 2/3, Step: 1, Loss: 7.243695259094238\nEpoch: 2/3, Step: 2, Loss: 7.223856449127197\nEpoch: 2/3, Step: 3, Loss: 7.164932727813721\nEpoch: 2/3, Step: 4, Loss: 7.183753490447998\nEpoch: 2/3, Step: 5, Loss: 7.188785076141357\nEpoch: 2/3, Step: 6, Loss: 7.126603126525879\nEpoch: 2/3, Step: 7, Loss: 7.142350196838379\nEpoch: 2/3, Step: 8, Loss: 7.128390312194824\nEpoch: 2/3, Step: 9, Loss: 7.119665145874023\nEpoch: 2/3, Step: 10, Loss: 7.092712879180908\nEpoch: 2/3, Step: 11, Loss: 7.095759868621826\nEpoch: 2/3, Step: 12, Loss: 7.049081325531006\nEpoch: 2/3, Step: 13, Loss: 7.034635543823242\nEpoch: 2/3, Step: 14, Loss: 7.0157670974731445\nEpoch: 2/3, Step: 15, Loss: 7.040457725524902\nEpoch: 2/3, Step: 16, Loss: 6.97285270690918\nEpoch: 2/3, Step: 17, Loss: 6.97888708114624\nEpoch: 2/3, Step: 18, Loss: 6.962117671966553\nEpoch: 2/3, Step: 19, Loss: 6.942529201507568\nEpoch: 2/3, Step: 20, Loss: 6.890547275543213\nEpoch: 2/3, Step: 21, Loss: 6.919500350952148\nEpoch: 2/3, Step: 22, Loss: 6.902340888977051\nEpoch: 2/3, Step: 23, Loss: 6.880086898803711\nEpoch: 2/3, Step: 24, Loss: 6.846684455871582\nEpoch: 2/3, Step: 25, Loss: 6.855042457580566\nEpoch: 2/3, Step: 26, Loss: 6.819039344787598\nEpoch: 2/3, Step: 27, Loss: 6.823531150817871\nEpoch: 2/3, Step: 28, Loss: 6.798237323760986\nEpoch: 2/3, Step: 29, Loss: 6.788008213043213\nEpoch: 2/3, Step: 30, Loss: 6.783342361450195\nEpoch: 2/3, Step: 31, Loss: 6.7682719230651855\nEpoch: 2/3, Step: 32, Loss: 6.751431465148926\nEpoch: 2/3, Step: 33, Loss: 6.749016284942627\nEpoch: 2/3, Step: 34, Loss: 6.73579740524292\nEpoch: 2/3, Step: 35, Loss: 6.705609321594238\nEpoch: 2/3, Step: 36, Loss: 6.671651840209961\nEpoch: 2/3, Step: 37, Loss: 6.678922653198242\nEpoch: 2/3, Step: 38, Loss: 6.6713457107543945\nEpoch: 2/3, Step: 39, Loss: 6.66525411605835\nEpoch: 2/3, Step: 40, Loss: 6.596367835998535\nEpoch: 2/3, Step: 41, Loss: 6.624523639678955\nEpoch: 2/3, Step: 42, Loss: 6.570672035217285\nEpoch: 2/3, Step: 43, Loss: 6.552794456481934\nEpoch: 2/3, Step: 44, Loss: 6.577075004577637\nEpoch: 2/3, Step: 45, Loss: 6.617122650146484\nEpoch: 2/3, Step: 46, Loss: 6.521844863891602\nEpoch: 2/3, Step: 47, Loss: 6.538998603820801\nEpoch: 2/3, Step: 48, Loss: 6.519711494445801\nEpoch: 2/3, Step: 49, Loss: 6.463767051696777\nEpoch: 2/3, Step: 50, Loss: 6.499827861785889\nEpoch: 2/3, Step: 51, Loss: 6.4856858253479\nEpoch: 2/3, Step: 52, Loss: 6.464856147766113\nEpoch: 2/3, Step: 53, Loss: 6.416810035705566\nEpoch: 2/3, Step: 54, Loss: 6.436773300170898\nEpoch: 2/3, Step: 55, Loss: 6.373676300048828\nEpoch: 2/3, Step: 56, Loss: 6.403632640838623\nEpoch: 2/3, Step: 57, Loss: 6.4459686279296875\nEpoch: 2/3, Step: 58, Loss: 6.477977752685547\nEpoch: 2/3, Step: 59, Loss: 6.330357551574707\nEpoch: 2/3, Step: 60, Loss: 6.36427640914917\nEpoch: 2/3, Step: 61, Loss: 6.316694259643555\nEpoch: 2/3, Step: 62, Loss: 6.289958477020264\nEpoch: 2/3, Step: 63, Loss: 6.311151504516602\nEpoch: 2/3, Step: 64, Loss: 6.337613105773926\nEpoch: 2/3, Step: 65, Loss: 6.300755977630615\nEpoch: 2/3, Step: 66, Loss: 6.222718715667725\nEpoch: 2/3, Step: 67, Loss: 6.220170974731445\nEpoch: 2/3, Step: 68, Loss: 6.209284782409668\nEpoch: 2/3, Step: 69, Loss: 6.206571578979492\nEpoch: 2/3, Step: 70, Loss: 6.2157487869262695\nEpoch: 2/3, Step: 71, Loss: 6.245072364807129\nEpoch: 2/3, Step: 72, Loss: 6.17720890045166\nEpoch: 2/3, Step: 73, Loss: 6.166243076324463\nEpoch: 2/3, Step: 74, Loss: 6.14687967300415\nEpoch: 2/3, Step: 75, Loss: 6.119991302490234\nEpoch: 2/3, Step: 76, Loss: 6.082202434539795\nEpoch: 2/3, Step: 77, Loss: 6.051495552062988\nEpoch: 2/3, Step: 78, Loss: 6.056193828582764\nEpoch: 2/3, Step: 79, Loss: 6.066560745239258\nEpoch: 2/3, Step: 80, Loss: 6.030570030212402\nEpoch: 2/3, Step: 81, Loss: 6.021482467651367\nEpoch: 2/3, Step: 82, Loss: 6.162021636962891\nEpoch: 2/3, Step: 83, Loss: 6.021265506744385\nEpoch: 2/3, Step: 84, Loss: 5.989282608032227\nEpoch: 2/3, Step: 85, Loss: 5.998199462890625\nEpoch: 2/3, Step: 86, Loss: 5.972604274749756\nEpoch: 2/3, Step: 87, Loss: 5.971240043640137\nEpoch: 2/3, Step: 88, Loss: 5.9647650718688965\nEpoch: 2/3, Step: 89, Loss: 5.961058139801025\nEpoch: 2/3, Step: 90, Loss: 5.902113914489746\nEpoch: 2/3, Step: 91, Loss: 5.876443386077881\nEpoch: 3/3, Step: 20, Loss: 5.471054553985596\nEpoch: 3/3, Step: 21, Loss: 5.511417388916016\nEpoch: 3/3, Step: 22, Loss: 5.476212978363037\nEpoch: 3/3, Step: 23, Loss: 5.451431751251221\nEpoch: 3/3, Step: 24, Loss: 5.44795036315918\nEpoch: 3/3, Step: 25, Loss: 5.415022850036621\nEpoch: 3/3, Step: 26, Loss: 5.404407024383545\nEpoch: 3/3, Step: 27, Loss: 5.406078815460205\nEpoch: 3/3, Step: 28, Loss: 5.3801493644714355\nEpoch: 3/3, Step: 29, Loss: 5.398564338684082\nEpoch: 3/3, Step: 30, Loss: 5.37206506729126\nEpoch: 3/3, Step: 31, Loss: 5.355684757232666\nEpoch: 3/3, Step: 32, Loss: 5.3421630859375\nEpoch: 3/3, Step: 33, Loss: 5.368460178375244\nEpoch: 3/3, Step: 34, Loss: 5.331188678741455\nEpoch: 3/3, Step: 35, Loss: 5.287280559539795\nEpoch: 3/3, Step: 36, Loss: 5.2442626953125\nEpoch: 3/3, Step: 37, Loss: 5.2622480392456055\nEpoch: 3/3, Step: 38, Loss: 5.24422550201416\nEpoch: 3/3, Step: 39, Loss: 5.261556625366211\nEpoch: 3/3, Step: 40, Loss: 5.186736106872559\nEpoch: 3/3, Step: 41, Loss: 5.238698482513428\nEpoch: 3/3, Step: 42, Loss: 5.17990779876709\nEpoch: 3/3, Step: 43, Loss: 5.128559112548828\nEpoch: 3/3, Step: 44, Loss: 5.189120769500732\nEpoch: 3/3, Step: 45, Loss: 5.273526668548584\nEpoch: 3/3, Step: 46, Loss: 5.132565975189209\nEpoch: 3/3, Step: 47, Loss: 5.158048152923584\nEpoch: 3/3, Step: 48, Loss: 5.142270088195801\nEpoch: 3/3, Step: 49, Loss: 5.074038028717041\nEpoch: 3/3, Step: 50, Loss: 5.125392436981201\nEpoch: 3/3, Step: 51, Loss: 5.103391170501709\nEpoch: 3/3, Step: 52, Loss: 5.112048625946045\nEpoch: 3/3, Step: 53, Loss: 5.02248477935791\nEpoch: 3/3, Step: 54, Loss: 5.0945539474487305\nEpoch: 3/3, Step: 55, Loss: 4.998267650604248\nEpoch: 3/3, Step: 56, Loss: 5.033320426940918\nEpoch: 3/3, Step: 57, Loss: 5.112659931182861\nEpoch: 3/3, Step: 58, Loss: 5.182616710662842\nEpoch: 3/3, Step: 59, Loss: 4.9845476150512695\nEpoch: 3/3, Step: 60, Loss: 5.015644073486328\nEpoch: 3/3, Step: 61, Loss: 4.985320091247559\nEpoch: 3/3, Step: 62, Loss: 4.929824352264404\nEpoch: 3/3, Step: 63, Loss: 4.9478044509887695\nEpoch: 3/3, Step: 64, Loss: 5.000545024871826\nEpoch: 3/3, Step: 65, Loss: 4.9553117752075195\nEpoch: 3/3, Step: 66, Loss: 4.877294063568115\nEpoch: 3/3, Step: 67, Loss: 4.859086036682129\nEpoch: 3/3, Step: 68, Loss: 4.85720157623291\nEpoch: 3/3, Step: 69, Loss: 4.844881534576416\nEpoch: 3/3, Step: 70, Loss: 4.888640880584717\nEpoch: 3/3, Step: 71, Loss: 4.966017723083496\nEpoch: 3/3, Step: 72, Loss: 4.84896993637085\nEpoch: 3/3, Step: 73, Loss: 4.829325199127197\nEpoch: 3/3, Step: 74, Loss: 4.823816299438477\nEpoch: 3/3, Step: 75, Loss: 4.772780895233154\nEpoch: 3/3, Step: 76, Loss: 4.746150493621826\nEpoch: 3/3, Step: 77, Loss: 4.719350814819336\nEpoch: 3/3, Step: 78, Loss: 4.715729713439941\nEpoch: 3/3, Step: 79, Loss: 4.75000524520874\nEpoch: 3/3, Step: 80, Loss: 4.70802640914917\nEpoch: 3/3, Step: 81, Loss: 4.700471878051758\nEpoch: 3/3, Step: 82, Loss: 4.911762714385986\nEpoch: 3/3, Step: 83, Loss: 4.730799198150635\nEpoch: 3/3, Step: 84, Loss: 4.67603063583374\nEpoch: 3/3, Step: 85, Loss: 4.700835227966309\nEpoch: 3/3, Step: 86, Loss: 4.695774078369141\nEpoch: 3/3, Step: 87, Loss: 4.676191806793213\nEpoch: 3/3, Step: 88, Loss: 4.653295993804932\nEpoch: 3/3, Step: 89, Loss: 4.677138328552246\nEpoch: 3/3, Step: 90, Loss: 4.587326526641846\nEpoch: 3/3, Step: 91, Loss: 4.5775017738342285\nEpoch: 3/3, Step: 92, Loss: 4.668580055236816\nEpoch: 3/3, Step: 93, Loss: 4.567760467529297\nEpoch: 3/3, Step: 94, Loss: 4.558079719543457\nEpoch: 3/3, Step: 95, Loss: 4.585484981536865\nEpoch: 3/3, Step: 96, Loss: 4.588350772857666\nEpoch: 3/3, Step: 97, Loss: 4.551905155181885\nEpoch: 3/3, Step: 98, Loss: 4.480876922607422\nEpoch: 3/3, Step: 99, Loss: 4.513561248779297\nEpoch 3 finished. Average Loss: 0.7329337753568377\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Function to perform inference on the fine-tuned model\ndef generate_response(prompt):\n    # Tokenize the input prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n    \n    # Move the inputs to the appropriate device (CPU or GPU)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    \n    # Put the model in evaluation mode\n    peft_model.eval()\n    \n    # Generate a response using the model (greedy decoding for simplicity)\n    with torch.no_grad():\n        outputs = peft_model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=50,\n            num_return_sequences=1,  # Generate only one sequence\n            do_sample=False          # Greedy decoding\n        )\n    \n    # Decode the generated tokens back into text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return generated_text\n\n# Sample prompt for testing (you can modify this with any other example)\nsample_prompt = \"Solve the following equation: 2x + 3 = 7. What is the value of x?\"\n\n# Generate response from the fine-tuned model\ngenerated_output = generate_response(sample_prompt)\n\n# Print the model's generated response\nprint(\"Prompt:\", sample_prompt)\nprint(\"Model's Response:\", generated_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T11:12:45.354880Z","iopub.execute_input":"2024-10-20T11:12:45.355301Z","iopub.status.idle":"2024-10-20T11:12:50.750054Z","shell.execute_reply.started":"2024-10-20T11:12:45.355262Z","shell.execute_reply":"2024-10-20T11:12:50.749123Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1755: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Solve the following equation: 2x + 3 = 7. What is the value of x?\nModel's Response: Solve the following equation: 2x + 3 = 7. What is the value of x?\n\nComment: @Joe, I'm not sure what you mean by \"the value of x\".\n\nComment: @Joe, I'm not sure what you mean by \"the value of x\".\n\nComment:\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}