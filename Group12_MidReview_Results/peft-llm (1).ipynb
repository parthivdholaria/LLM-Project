{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install transformers datasets peft torch\n! pip install -U bitsandbytes\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T20:27:50.679322Z","iopub.execute_input":"2024-10-19T20:27:50.679728Z","iopub.status.idle":"2024-10-19T20:28:19.518729Z","shell.execute_reply.started":"2024-10-19T20:27:50.679688Z","shell.execute_reply":"2024-10-19T20:28:19.517797Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\nfrom datasets import load_dataset\nfrom peft import PromptTuningConfig, get_peft_model\n\n# Load the model and tokenizer with 4-bit quantization enabled and CPU offloading\nmodel_name = \"google/gemma-7b-it\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    use_auth_token=\"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\",\n    load_in_4bit=True,  # Enable 4-bit quantization\n    device_map=\"cuda\"  # Automatically map layers to available devices (CPU & GPU)  # Offload some layers to CPU in 32-bit precision\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_auth_token=\"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\"\n)\n\n# Define the Prompt Tuning configuration\npeft_config = PromptTuningConfig(\n    task_type=\"CAUSAL_LM\",    # Set the task type for causal language modeling\n    num_virtual_tokens=20     # Number of virtual tokens to tune (instead of prompt_length)\n)\n\n# Apply prompt tuning to the model\npeft_model = get_peft_model(model, peft_config)\n\n# Load the SVAMP dataset\ndataset = load_dataset(\"ChilleD/SVAMP\", split='train')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T20:28:33.112114Z","iopub.execute_input":"2024-10-19T20:28:33.113181Z","iopub.status.idle":"2024-10-19T20:31:31.026282Z","shell.execute_reply.started":"2024-10-19T20:28:33.113120Z","shell.execute_reply":"2024-10-19T20:31:31.025315Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b1540c3aee447a8e138a892ecf5479"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"350d89c469354127b0003b1319220fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7696c093238641a9adcdbb03396fa676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a07e3eee4884da9bda8c75d812caf3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f166783092e447a5a07ed35f6118fd07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7cc61996f904faeb7df46c63be36b62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098cfb3804384a26a96048047929b539"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cc47c99820409390c8fa157f548128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18626e1dabeb4d3387839acc1a65eb54"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8f3122844e4007a7e77c6e7c7461de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97052a75e47d422fab901952a66057ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11217bbcf5f74f11a2b865361880013f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d3febe330747df985eee77311d798b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/675 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a15211a045de48bf8fbab9f5e3c6a442"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/111k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4957b2fb9ae440f9fbe7e9e807c50c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/54.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09f61673bd1943dca11459b395a20b1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aec8c87dfeeb463abc2c1af12890ed83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c7b8ba08e0445189557b84ece759fbf"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:23:50.985159Z","iopub.execute_input":"2024-10-19T19:23:50.985579Z","iopub.status.idle":"2024-10-19T19:23:50.992198Z","shell.execute_reply.started":"2024-10-19T19:23:50.985542Z","shell.execute_reply":"2024-10-19T19:23:50.991242Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'145'"},"metadata":{}}]},{"cell_type":"code","source":"# Preprocess the dataset\n# Set the maximum length for tokenization\nMAX_LENGTH = 512  # Adjust this depending on the model's input size limit (common is 512 for many models)\n\n# Preprocess the dataset with truncation and padding to MAX_LENGTH\n# Set the maximum length for tokenization\nMAX_LENGTH = 512  # Adjust this depending on the model's input size limit\n\n# Preprocess function as defined previously\ndef preprocess_function(example):\n    inputs = example['Body'] + example['Question']\n    targets = example['Answer']\n    \n    # Tokenize the inputs and targets with max_length\n    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n    labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n    \n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Prepare the list to hold preprocessed data\npreprocessed_data = []\n\n# Loop through each sample in the dataset\nfor example in dataset:\n    # Apply the preprocess function to each sample\n    preprocessed_example = preprocess_function(example)\n    preprocessed_data.append(preprocessed_example)\n\n# Convert the list of preprocessed data to the required format for training\n\nprint(len(preprocessed_data))\nfrom datasets import Dataset\ntrain_dataset = Dataset.from_dict({key: [example[key] for example in preprocessed_data] for key in preprocessed_data[0].keys()})\n\n# Now `train_dataset` can be used in training\n\n\n\n# Define the optimizer and scheduler\noptimizer = AdamW(peft_model.parameters(), lr=5e-5)\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataset)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Move model to GPU if available, CPU offloading is handled by `device_map`\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\npeft_model.to(device)\n\n# Custom training loop\nfor epoch in range(num_epochs):\n    peft_model.train()  # Set the model to training mode\n    total_loss = 0\n    \n    for idx in range(100):\n        batch = train_dataset[idx]\n#         print(\"Step: \", step)\n#         print(\"\\n\")\n#         print(\"Batch: \", batch)\n        # Move inputs and labels to device\n        input_ids = torch.tensor(batch['input_ids']).unsqueeze(0).to(device)\n        labels = torch.tensor(batch['labels']).unsqueeze(0).to(device)\n\n        # Forward pass\n        outputs = peft_model(input_ids=input_ids, labels=labels)\n        loss = outputs.loss\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()  # Update learning rate\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n        # Print progress\n        print(f\"Epoch: {epoch + 1}/{num_epochs}, Step: {idx}, Loss: {loss.item()}\")\n\n    avg_loss = total_loss / len(train_dataset)\n    print(f\"Epoch {epoch + 1} finished. Average Loss: {avg_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T20:33:01.577498Z","iopub.execute_input":"2024-10-19T20:33:01.578763Z","iopub.status.idle":"2024-10-19T20:50:19.025094Z","shell.execute_reply.started":"2024-10-19T20:33:01.578709Z","shell.execute_reply":"2024-10-19T20:50:19.024081Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"700\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/3, Step: 0, Loss: 31.974206924438477\nEpoch: 1/3, Step: 1, Loss: 31.89295196533203\nEpoch: 1/3, Step: 2, Loss: 31.56702995300293\nEpoch: 1/3, Step: 3, Loss: 32.13707733154297\nEpoch: 1/3, Step: 4, Loss: 31.401500701904297\nEpoch: 1/3, Step: 5, Loss: 31.85927963256836\nEpoch: 1/3, Step: 6, Loss: 31.562692642211914\nEpoch: 1/3, Step: 7, Loss: 31.47697639465332\nEpoch: 1/3, Step: 8, Loss: 31.58785057067871\nEpoch: 1/3, Step: 9, Loss: 31.33023452758789\nEpoch: 1/3, Step: 10, Loss: 31.607744216918945\nEpoch: 1/3, Step: 11, Loss: 31.703643798828125\nEpoch: 1/3, Step: 12, Loss: 31.2880802154541\nEpoch: 1/3, Step: 13, Loss: 31.447614669799805\nEpoch: 1/3, Step: 14, Loss: 31.383113861083984\nEpoch: 1/3, Step: 15, Loss: 31.323991775512695\nEpoch: 1/3, Step: 16, Loss: 31.25873565673828\nEpoch: 1/3, Step: 17, Loss: 31.858776092529297\nEpoch: 1/3, Step: 18, Loss: 31.54234504699707\nEpoch: 1/3, Step: 19, Loss: 31.01314353942871\nEpoch: 1/3, Step: 20, Loss: 31.517169952392578\nEpoch: 1/3, Step: 21, Loss: 31.793981552124023\nEpoch: 1/3, Step: 22, Loss: 31.34901237487793\nEpoch: 1/3, Step: 23, Loss: 31.42226791381836\nEpoch: 1/3, Step: 24, Loss: 31.5466365814209\nEpoch: 1/3, Step: 25, Loss: 31.047069549560547\nEpoch: 1/3, Step: 26, Loss: 31.285728454589844\nEpoch: 1/3, Step: 27, Loss: 31.323047637939453\nEpoch: 1/3, Step: 28, Loss: 31.19861602783203\nEpoch: 1/3, Step: 29, Loss: 31.89769744873047\nEpoch: 1/3, Step: 30, Loss: 31.08685302734375\nEpoch: 1/3, Step: 31, Loss: 31.29368019104004\nEpoch: 1/3, Step: 32, Loss: 31.339492797851562\nEpoch: 1/3, Step: 33, Loss: 31.53005599975586\nEpoch: 1/3, Step: 34, Loss: 31.49353790283203\nEpoch: 1/3, Step: 35, Loss: 31.09552574157715\nEpoch: 1/3, Step: 36, Loss: 30.976051330566406\nEpoch: 1/3, Step: 37, Loss: 30.77288246154785\nEpoch: 1/3, Step: 38, Loss: 30.782917022705078\nEpoch: 1/3, Step: 39, Loss: 30.812772750854492\nEpoch: 1/3, Step: 40, Loss: 30.75116539001465\nEpoch: 1/3, Step: 41, Loss: 31.086332321166992\nEpoch: 1/3, Step: 42, Loss: 31.283315658569336\nEpoch: 1/3, Step: 43, Loss: 30.616640090942383\nEpoch: 1/3, Step: 44, Loss: 31.03179931640625\nEpoch: 1/3, Step: 45, Loss: 31.534067153930664\nEpoch: 1/3, Step: 46, Loss: 30.767812728881836\nEpoch: 1/3, Step: 47, Loss: 31.307218551635742\nEpoch: 1/3, Step: 48, Loss: 30.894582748413086\nEpoch: 1/3, Step: 49, Loss: 31.16341209411621\nEpoch: 1/3, Step: 50, Loss: 31.071996688842773\nEpoch: 1/3, Step: 51, Loss: 30.600505828857422\nEpoch: 1/3, Step: 52, Loss: 31.094877243041992\nEpoch: 1/3, Step: 53, Loss: 30.80438804626465\nEpoch: 1/3, Step: 54, Loss: 31.217239379882812\nEpoch: 1/3, Step: 55, Loss: 30.86659049987793\nEpoch: 1/3, Step: 56, Loss: 30.722400665283203\nEpoch: 1/3, Step: 57, Loss: 31.14871597290039\nEpoch: 1/3, Step: 58, Loss: 32.158145904541016\nEpoch: 1/3, Step: 59, Loss: 31.334123611450195\nEpoch: 1/3, Step: 60, Loss: 30.785226821899414\nEpoch: 1/3, Step: 61, Loss: 31.205093383789062\nEpoch: 1/3, Step: 62, Loss: 30.885391235351562\nEpoch: 1/3, Step: 63, Loss: 30.544687271118164\nEpoch: 1/3, Step: 64, Loss: 30.898597717285156\nEpoch: 1/3, Step: 65, Loss: 30.91876983642578\nEpoch: 1/3, Step: 66, Loss: 30.789562225341797\nEpoch: 1/3, Step: 67, Loss: 30.52831268310547\nEpoch: 1/3, Step: 68, Loss: 30.6662654876709\nEpoch: 1/3, Step: 69, Loss: 30.43131446838379\nEpoch: 1/3, Step: 70, Loss: 31.032379150390625\nEpoch: 1/3, Step: 71, Loss: 31.648324966430664\nEpoch: 1/3, Step: 72, Loss: 30.639116287231445\nEpoch: 1/3, Step: 73, Loss: 30.70366668701172\nEpoch: 1/3, Step: 74, Loss: 30.629623413085938\nEpoch: 1/3, Step: 75, Loss: 30.27720069885254\nEpoch: 1/3, Step: 76, Loss: 30.312284469604492\nEpoch: 1/3, Step: 77, Loss: 30.34320831298828\nEpoch: 1/3, Step: 78, Loss: 30.388174057006836\nEpoch: 1/3, Step: 79, Loss: 30.376955032348633\nEpoch: 1/3, Step: 80, Loss: 30.550495147705078\nEpoch: 1/3, Step: 81, Loss: 30.455913543701172\nEpoch: 1/3, Step: 82, Loss: 31.79913902282715\nEpoch: 1/3, Step: 83, Loss: 31.155710220336914\nEpoch: 1/3, Step: 84, Loss: 30.236330032348633\nEpoch: 1/3, Step: 85, Loss: 30.788541793823242\nEpoch: 1/3, Step: 86, Loss: 30.871726989746094\nEpoch: 1/3, Step: 87, Loss: 30.483102798461914\nEpoch: 1/3, Step: 88, Loss: 30.192543029785156\nEpoch: 1/3, Step: 89, Loss: 30.73821258544922\nEpoch: 1/3, Step: 90, Loss: 30.223957061767578\nEpoch: 1/3, Step: 91, Loss: 30.1508731842041\nEpoch: 1/3, Step: 92, Loss: 31.194652557373047\nEpoch: 1/3, Step: 93, Loss: 30.72073745727539\nEpoch: 1/3, Step: 94, Loss: 30.650737762451172\nEpoch: 1/3, Step: 95, Loss: 30.541105270385742\nEpoch: 1/3, Step: 96, Loss: 30.46592140197754\nEpoch: 1/3, Step: 97, Loss: 30.013851165771484\nEpoch: 1/3, Step: 98, Loss: 30.073366165161133\nEpoch: 1/3, Step: 99, Loss: 30.231578826904297\nEpoch 1 finished. Average Loss: 4.433971380506243\nEpoch: 2/3, Step: 0, Loss: 30.532367706298828\nEpoch: 2/3, Step: 1, Loss: 30.45009994506836\nEpoch: 2/3, Step: 2, Loss: 30.101511001586914\nEpoch: 2/3, Step: 3, Loss: 30.733083724975586\nEpoch: 2/3, Step: 4, Loss: 29.946269989013672\nEpoch: 2/3, Step: 5, Loss: 30.465023040771484\nEpoch: 2/3, Step: 6, Loss: 30.149433135986328\nEpoch: 2/3, Step: 7, Loss: 30.032745361328125\nEpoch: 2/3, Step: 8, Loss: 30.175249099731445\nEpoch: 2/3, Step: 9, Loss: 29.891042709350586\nEpoch: 2/3, Step: 10, Loss: 30.19289207458496\nEpoch: 2/3, Step: 11, Loss: 30.294702529907227\nEpoch: 2/3, Step: 12, Loss: 29.859973907470703\nEpoch: 2/3, Step: 13, Loss: 30.052722930908203\nEpoch: 2/3, Step: 14, Loss: 29.97823715209961\nEpoch: 2/3, Step: 15, Loss: 29.928524017333984\nEpoch: 2/3, Step: 16, Loss: 29.868358612060547\nEpoch: 2/3, Step: 17, Loss: 30.513620376586914\nEpoch: 2/3, Step: 18, Loss: 30.173206329345703\nEpoch: 2/3, Step: 19, Loss: 29.62542724609375\nEpoch: 2/3, Step: 20, Loss: 30.17776870727539\nEpoch: 2/3, Step: 21, Loss: 30.463184356689453\nEpoch: 2/3, Step: 22, Loss: 29.997140884399414\nEpoch: 2/3, Step: 23, Loss: 30.07921028137207\nEpoch: 2/3, Step: 24, Loss: 30.21910858154297\nEpoch: 2/3, Step: 25, Loss: 29.69947624206543\nEpoch: 2/3, Step: 26, Loss: 29.965545654296875\nEpoch: 2/3, Step: 27, Loss: 30.005435943603516\nEpoch: 2/3, Step: 28, Loss: 29.866844177246094\nEpoch: 2/3, Step: 29, Loss: 30.601417541503906\nEpoch: 2/3, Step: 30, Loss: 29.775175094604492\nEpoch: 2/3, Step: 31, Loss: 29.988765716552734\nEpoch: 2/3, Step: 32, Loss: 30.049545288085938\nEpoch: 2/3, Step: 33, Loss: 30.260581970214844\nEpoch: 2/3, Step: 34, Loss: 30.21935272216797\nEpoch: 2/3, Step: 35, Loss: 29.800662994384766\nEpoch: 2/3, Step: 36, Loss: 29.666309356689453\nEpoch: 2/3, Step: 37, Loss: 29.488056182861328\nEpoch: 2/3, Step: 38, Loss: 29.477184295654297\nEpoch: 2/3, Step: 39, Loss: 29.533830642700195\nEpoch: 2/3, Step: 40, Loss: 29.46487808227539\nEpoch: 2/3, Step: 41, Loss: 29.828845977783203\nEpoch: 2/3, Step: 42, Loss: 30.03606605529785\nEpoch: 2/3, Step: 43, Loss: 29.32130241394043\nEpoch: 2/3, Step: 44, Loss: 29.788883209228516\nEpoch: 2/3, Step: 45, Loss: 30.340373992919922\nEpoch: 2/3, Step: 46, Loss: 29.514644622802734\nEpoch: 2/3, Step: 47, Loss: 30.08867645263672\nEpoch: 2/3, Step: 48, Loss: 29.672170639038086\nEpoch: 2/3, Step: 49, Loss: 29.937875747680664\nEpoch: 2/3, Step: 50, Loss: 29.855226516723633\nEpoch: 2/3, Step: 51, Loss: 29.37499237060547\nEpoch: 2/3, Step: 52, Loss: 29.89982032775879\nEpoch: 2/3, Step: 53, Loss: 29.581932067871094\nEpoch: 2/3, Step: 54, Loss: 30.039207458496094\nEpoch: 2/3, Step: 55, Loss: 29.658737182617188\nEpoch: 2/3, Step: 56, Loss: 29.530210494995117\nEpoch: 2/3, Step: 57, Loss: 29.986438751220703\nEpoch: 2/3, Step: 58, Loss: 31.0325870513916\nEpoch: 2/3, Step: 59, Loss: 30.162965774536133\nEpoch: 2/3, Step: 60, Loss: 29.619556427001953\nEpoch: 2/3, Step: 61, Loss: 30.05198097229004\nEpoch: 2/3, Step: 62, Loss: 29.700464248657227\nEpoch: 2/3, Step: 63, Loss: 29.364212036132812\nEpoch: 2/3, Step: 64, Loss: 29.7379093170166\nEpoch: 2/3, Step: 65, Loss: 29.766326904296875\nEpoch: 2/3, Step: 66, Loss: 29.637121200561523\nEpoch: 2/3, Step: 67, Loss: 29.370464324951172\nEpoch: 2/3, Step: 68, Loss: 29.521102905273438\nEpoch: 2/3, Step: 69, Loss: 29.277894973754883\nEpoch: 2/3, Step: 70, Loss: 29.901689529418945\nEpoch: 2/3, Step: 71, Loss: 30.53987693786621\nEpoch: 2/3, Step: 72, Loss: 29.526037216186523\nEpoch: 2/3, Step: 73, Loss: 29.576904296875\nEpoch: 2/3, Step: 74, Loss: 29.522310256958008\nEpoch: 2/3, Step: 75, Loss: 29.133012771606445\nEpoch: 2/3, Step: 76, Loss: 29.178529739379883\nEpoch: 2/3, Step: 77, Loss: 29.20939826965332\nEpoch: 2/3, Step: 78, Loss: 29.25963020324707\nEpoch: 2/3, Step: 79, Loss: 29.261520385742188\nEpoch: 2/3, Step: 80, Loss: 29.4366512298584\nEpoch: 2/3, Step: 81, Loss: 29.33880615234375\nEpoch: 2/3, Step: 82, Loss: 30.76428985595703\nEpoch: 2/3, Step: 83, Loss: 30.08113670349121\nEpoch: 2/3, Step: 84, Loss: 29.14063262939453\nEpoch: 2/3, Step: 85, Loss: 29.712858200073242\nEpoch: 2/3, Step: 86, Loss: 29.800331115722656\nEpoch: 2/3, Step: 87, Loss: 29.41383171081543\nEpoch: 2/3, Step: 88, Loss: 29.10489845275879\nEpoch: 2/3, Step: 89, Loss: 29.675182342529297\nEpoch: 2/3, Step: 90, Loss: 29.132234573364258\nEpoch: 2/3, Step: 91, Loss: 29.074058532714844\nEpoch: 2/3, Step: 92, Loss: 30.170961380004883\nEpoch: 2/3, Step: 93, Loss: 29.667522430419922\nEpoch: 2/3, Step: 94, Loss: 29.601709365844727\nEpoch: 2/3, Step: 95, Loss: 29.488292694091797\nEpoch: 2/3, Step: 96, Loss: 29.4160213470459\nEpoch: 2/3, Step: 97, Loss: 28.96640968322754\nEpoch: 2/3, Step: 98, Loss: 28.998226165771484\nEpoch: 2/3, Step: 99, Loss: 29.173721313476562\nEpoch 2 finished. Average Loss: 4.258183773585729\nEpoch: 3/3, Step: 0, Loss: 29.499771118164062\nEpoch: 3/3, Step: 1, Loss: 29.40911102294922\nEpoch: 3/3, Step: 2, Loss: 29.0520076751709\nEpoch: 3/3, Step: 3, Loss: 29.726884841918945\nEpoch: 3/3, Step: 4, Loss: 28.908239364624023\nEpoch: 3/3, Step: 5, Loss: 29.455394744873047\nEpoch: 3/3, Step: 6, Loss: 29.13054847717285\nEpoch: 3/3, Step: 7, Loss: 28.995820999145508\nEpoch: 3/3, Step: 8, Loss: 29.157215118408203\nEpoch: 3/3, Step: 9, Loss: 28.85995101928711\nEpoch: 3/3, Step: 10, Loss: 29.169261932373047\nEpoch: 3/3, Step: 11, Loss: 29.279823303222656\nEpoch: 3/3, Step: 12, Loss: 28.835662841796875\nEpoch: 3/3, Step: 13, Loss: 29.044780731201172\nEpoch: 3/3, Step: 14, Loss: 28.965402603149414\nEpoch: 3/3, Step: 15, Loss: 28.914709091186523\nEpoch: 3/3, Step: 16, Loss: 28.870141983032227\nEpoch: 3/3, Step: 17, Loss: 29.550395965576172\nEpoch: 3/3, Step: 18, Loss: 29.17862319946289\nEpoch: 3/3, Step: 19, Loss: 28.625825881958008\nEpoch: 3/3, Step: 20, Loss: 29.2051944732666\nEpoch: 3/3, Step: 21, Loss: 29.485164642333984\nEpoch: 3/3, Step: 22, Loss: 29.00023651123047\nEpoch: 3/3, Step: 23, Loss: 29.10254669189453\nEpoch: 3/3, Step: 24, Loss: 29.258262634277344\nEpoch: 3/3, Step: 25, Loss: 28.727083206176758\nEpoch: 3/3, Step: 26, Loss: 29.001962661743164\nEpoch: 3/3, Step: 27, Loss: 29.04781723022461\nEpoch: 3/3, Step: 28, Loss: 28.899188995361328\nEpoch: 3/3, Step: 29, Loss: 29.656742095947266\nEpoch: 3/3, Step: 30, Loss: 28.818931579589844\nEpoch: 3/3, Step: 31, Loss: 29.03717613220215\nEpoch: 3/3, Step: 32, Loss: 29.094602584838867\nEpoch: 3/3, Step: 33, Loss: 29.319780349731445\nEpoch: 3/3, Step: 34, Loss: 29.281280517578125\nEpoch: 3/3, Step: 35, Loss: 28.855148315429688\nEpoch: 3/3, Step: 36, Loss: 28.704463958740234\nEpoch: 3/3, Step: 37, Loss: 28.535947799682617\nEpoch: 3/3, Step: 38, Loss: 28.527332305908203\nEpoch: 3/3, Step: 39, Loss: 28.599445343017578\nEpoch: 3/3, Step: 40, Loss: 28.527925491333008\nEpoch: 3/3, Step: 41, Loss: 28.912097930908203\nEpoch: 3/3, Step: 42, Loss: 29.115398406982422\nEpoch: 3/3, Step: 43, Loss: 28.370100021362305\nEpoch: 3/3, Step: 44, Loss: 28.879039764404297\nEpoch: 3/3, Step: 45, Loss: 29.460277557373047\nEpoch: 3/3, Step: 46, Loss: 28.58781623840332\nEpoch: 3/3, Step: 47, Loss: 29.180105209350586\nEpoch: 3/3, Step: 48, Loss: 28.76151466369629\nEpoch: 3/3, Step: 49, Loss: 29.024734497070312\nEpoch: 3/3, Step: 50, Loss: 28.966176986694336\nEpoch: 3/3, Step: 51, Loss: 28.467405319213867\nEpoch: 3/3, Step: 52, Loss: 29.019330978393555\nEpoch: 3/3, Step: 53, Loss: 28.669662475585938\nEpoch: 3/3, Step: 54, Loss: 29.171079635620117\nEpoch: 3/3, Step: 55, Loss: 28.756118774414062\nEpoch: 3/3, Step: 56, Loss: 28.644861221313477\nEpoch: 3/3, Step: 57, Loss: 29.118846893310547\nEpoch: 3/3, Step: 58, Loss: 30.20131492614746\nEpoch: 3/3, Step: 59, Loss: 29.28789520263672\nEpoch: 3/3, Step: 60, Loss: 28.746612548828125\nEpoch: 3/3, Step: 61, Loss: 29.180370330810547\nEpoch: 3/3, Step: 62, Loss: 28.821386337280273\nEpoch: 3/3, Step: 63, Loss: 28.488021850585938\nEpoch: 3/3, Step: 64, Loss: 28.878232955932617\nEpoch: 3/3, Step: 65, Loss: 28.902820587158203\nEpoch: 3/3, Step: 66, Loss: 28.768205642700195\nEpoch: 3/3, Step: 67, Loss: 28.499155044555664\nEpoch: 3/3, Step: 68, Loss: 28.659709930419922\nEpoch: 3/3, Step: 69, Loss: 28.405054092407227\nEpoch: 3/3, Step: 70, Loss: 29.060029983520508\nEpoch: 3/3, Step: 71, Loss: 29.71173667907715\nEpoch: 3/3, Step: 72, Loss: 28.67731475830078\nEpoch: 3/3, Step: 73, Loss: 28.72028923034668\nEpoch: 3/3, Step: 74, Loss: 28.677492141723633\nEpoch: 3/3, Step: 75, Loss: 28.278505325317383\nEpoch: 3/3, Step: 76, Loss: 28.320817947387695\nEpoch: 3/3, Step: 77, Loss: 28.351688385009766\nEpoch: 3/3, Step: 78, Loss: 28.402233123779297\nEpoch: 3/3, Step: 79, Loss: 28.41140365600586\nEpoch: 3/3, Step: 80, Loss: 28.59705924987793\nEpoch: 3/3, Step: 81, Loss: 28.49081039428711\nEpoch: 3/3, Step: 82, Loss: 29.97559356689453\nEpoch: 3/3, Step: 83, Loss: 29.26270866394043\nEpoch: 3/3, Step: 84, Loss: 28.310699462890625\nEpoch: 3/3, Step: 85, Loss: 28.884857177734375\nEpoch: 3/3, Step: 86, Loss: 28.99805450439453\nEpoch: 3/3, Step: 87, Loss: 28.586050033569336\nEpoch: 3/3, Step: 88, Loss: 28.274580001831055\nEpoch: 3/3, Step: 89, Loss: 28.85369110107422\nEpoch: 3/3, Step: 90, Loss: 28.30019760131836\nEpoch: 3/3, Step: 91, Loss: 28.24331283569336\nEpoch: 3/3, Step: 92, Loss: 29.377197265625\nEpoch: 3/3, Step: 93, Loss: 28.853294372558594\nEpoch: 3/3, Step: 94, Loss: 28.80177879333496\nEpoch: 3/3, Step: 95, Loss: 28.68625831604004\nEpoch: 3/3, Step: 96, Loss: 28.61867904663086\nEpoch: 3/3, Step: 97, Loss: 28.156064987182617\nEpoch: 3/3, Step: 98, Loss: 28.186290740966797\nEpoch: 3/3, Step: 99, Loss: 28.371660232543945\nEpoch 3 finished. Average Loss: 4.128142144339425\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example inference after training\npeft_model.eval()\nsample_prompt = \"Solve the equation: 2x + 3 = 7.\"\n\n# Tokenize the input prompt\ninput_ids = tokenizer.encode(sample_prompt, return_tensors=\"pt\").to(device)\n\n# Generate the output with max_length\ngenerated_output = peft_model.generate(\n    input_ids,\n    max_length=512,  # Set the maximum number of tokens in the generated output\n    num_return_sequences=1,  # Generate one response\n    no_repeat_ngram_size=2,  # Optional: prevent repetition\n    do_sample=False  # Use greedy decoding for deterministic output\n)\n\n# Decode and print the generated text\nprint(tokenizer.decode(generated_output[0], skip_special_tokens=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example inference after training\npeft_model.eval()\nsample_prompt = \"Solve the equation: 2x + 3 = 7.\"\n\n# Tokenize the input prompt\ninput_ids = tokenizer.encode(sample_prompt, return_tensors=\"pt\").to(device)\n\n# Generate the output with max_length\ngenerated_output = peft_model.generate(\n    input_ids,\n    max_length=512,  # Set the maximum number of tokens in the generated output\n    num_return_sequences=1,  # Generate one response\n    no_repeat_ngram_size=2,  # Optional: prevent repetition\n    do_sample=False  # Use greedy decoding for deterministic output\n)\n\n# Decode and print the generated text\nprint(tokenizer.decode(generated_output[0], skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T20:20:42.337879Z","iopub.execute_input":"2024-10-19T20:20:42.338834Z","iopub.status.idle":"2024-10-19T20:21:06.551174Z","shell.execute_reply.started":"2024-10-19T20:20:42.338789Z","shell.execute_reply":"2024-10-19T20:21:06.550182Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":" autorytatywna (2018)\n\nSure, here is a summary of the paper \"The Future of Humanity in Space\" by Robert Zubrin:\n\n**Summary:**\n\nIn his paper, \"Future of humanity in space,\" Robert A. Zubin explores the potential for humanity to expand its presence beyond Earth through space colonization. He argues that space exploration is not simply a scientific endeavor but also a practical solution to many of our most pressing problems.\n\nZubrin identifies several key challenges that must be overcome before humanity can successfully colonize space. These include: \n\n* **Energy production:** We need to develop new energy sources that are more efficient than fossil fuels.\n*  **Food production**: We must develop ways to produce food in large quantities in a space environment. \n * **Water supply:** Humans require a significant amount of water for survival, and we must find ways of extracting water from the Moon and Mars.  \n  ***Transportation:** Developing affordable and reliable transportation systems for space travel is essential.\n\n\nZubin also discusses the ethical implications of colonizing space, such as the impact on Earth's ecosystem and the possibility of conflict between colonists. To address these concerns, he advocates for a collaborative approach to space development that emphasizes sustainability and inclusivity.   \n\nOverall, Zublin' vision for the future of humankind is one where we expand our presence in the cosmos through exploration and colonization, while ensuring that this expansion is sustainable and beneficial to all. His paper provides a thought-provoking exploration of this topic and offers a compelling case for human expansion beyond our planet.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Ensure the model is in evaluation mode\nmodel.eval()\n\n# Function to perform inference on the pre-trained model\ndef generate_response(prompt, max_length=1024):\n    # Tokenize the input prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n    \n    # Move the inputs to the appropriate device (CPU or GPU)\n    input_ids = inputs['input_ids'].to(\"cuda\")\n    attention_mask = inputs['attention_mask'].to(\"cuda\")\n    \n    # Generate a response using the model\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=max_length,        # Max length for the generated response\n            num_return_sequences=1,       # Generate only one sequence\n            do_sample=False,              # Greedy decoding\n            temperature=0.7,              # Control randomness (lower value for more deterministic output)\n            top_k=50,                     # Limit the sampling pool for more likely tokens\n            no_repeat_ngram_size=2        # Avoid repeating the same n-grams\n        )\n    \n    # Decode the generated tokens back into text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return generated_text\n\n# Sample prompt for testing\nsample_prompt = \"Solve the following equation: 3x + 4 = 10. What is the value of x?\"\n\n# Generate response from the pre-trained model\ngenerated_output = generate_response(sample_prompt)\n\n# Print the model's generated response\nprint(\"Prompt:\", sample_prompt)\nprint(\"Model's Response:\", generated_output)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T20:12:13.637339Z","iopub.execute_input":"2024-10-19T20:12:13.638093Z","iopub.status.idle":"2024-10-19T20:12:33.122512Z","shell.execute_reply.started":"2024-10-19T20:12:13.638033Z","shell.execute_reply":"2024-10-19T20:12:33.121580Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Solve the following equation: 3x + 4 = 10. What is the value of x?\nModel's Response: Solve the following equation: 3x + 4 = 10. What is the value of x?\n\n**Here is how to solve the equation:**\n\n1. Subtract 2 from both sides:\n3X +4 -2 =  12 - 0 =11\n\n2. Divide both side by 5:\n\nx =\n\nPlease provide the answer below:\n\n\n**Answer:**\n\n\n```\n\n```\nx=2\n```\n\n\nPlease let me know if this is correct.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}