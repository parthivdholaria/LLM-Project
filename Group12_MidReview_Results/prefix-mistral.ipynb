{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install transformers datasets peft torch\n! pip install -U bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, get_scheduler\nfrom datasets import load_dataset\n\n\n# Load the model and tokenizer\nmodel_name = \"mistralai/Mistral-7B-v0.1\"  # Replace with your desired model\ntokenizer = AutoTokenizer.from_pretrained(model_name,use_auth_token=\"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\",load_in_4bit=True,device_map=\"cuda\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name,use_auth_token=\"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\",load_in_4bit=True,device_map=\"cuda\")\n\n# Prefix tuning parameters\nprefix_length = 10  # Number of prefix tokens\nhidden_size = model.config.hidden_size\n\n# Wrap the model with PrefixTuning\n\n\n# Tokenization and dataset preprocessing\nMAX_LENGTH = 512\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n\ndef preprocess_function(example):\n    inputs = example['Body'] + example['Question']\n    targets = example['Answer']\n    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n    labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Load and preprocess dataset\ndataset = load_dataset(\"ChilleD/SVAMP\", split='train')\npreprocessed_data = [preprocess_function(example) for example in dataset]\n\nfrom datasets import Dataset\ntrain_dataset = Dataset.from_dict({key: [example[key] for example in preprocessed_data] for key in preprocessed_data[0].keys()})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PrefixTuning(nn.Module):\n    def __init__(self, model, prefix_length, hidden_size, dtype=torch.float16, device='cuda'):\n        super(PrefixTuning, self).__init__()\n        self.model = model\n        self.prefix_length = prefix_length\n        self.hidden_size = hidden_size\n\n        # Initialize the trainable prefix embeddings with the correct dtype and device\n        self.prefix_embedding = nn.Parameter(torch.randn(prefix_length, hidden_size, dtype=dtype, device=device)*0.01)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        # Extract model's embeddings for the input tokens\n        input_embeddings = self.model.get_input_embeddings()(input_ids)\n\n        # Use the prefix embeddings directly (already set to the correct dtype and device)\n        prefix_embeddings = self.prefix_embedding.unsqueeze(0).expand(input_embeddings.size(0), -1, -1)\n\n        # Concatenate the prefix with the input embeddings\n        extended_embeddings = torch.cat((prefix_embeddings, input_embeddings), dim=1)\n\n        # Adjust attention mask for the prefix\n        if attention_mask is not None:\n            prefix_attention_mask = torch.ones((input_embeddings.size(0), self.prefix_length), device=attention_mask.device, dtype=attention_mask.dtype)\n            extended_attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n        else:\n            extended_attention_mask = None\n\n        # Pad labels with -100 to ignore prefix tokens during loss computation\n        if labels is not None:\n            # Pad the labels with -100 (ignore index for the loss) to account for the prefix tokens\n            prefix_padding = torch.full((labels.size(0), self.prefix_length), -100, dtype=labels.dtype, device=labels.device)\n            extended_labels = torch.cat((prefix_padding, labels), dim=1)\n        else:\n            extended_labels = labels\n\n        # Forward pass through the model with the modified embeddings and labels\n        outputs = self.model(\n            inputs_embeds=extended_embeddings,\n            attention_mask=extended_attention_mask,\n            labels=extended_labels,\n        )\n        return outputs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Initialize the PrefixTuning model\nprefix_tuning_model = PrefixTuning(\n    model=model, \n    prefix_length=10, \n    hidden_size=model.config.hidden_size, \n    dtype=torch.float16,  # Set to the dtype your model is using (e.g., float16 for mixed precision)\n    device='cuda'  # Set the device accordingly\n)\noptimizer = AdamW(prefix_tuning_model.parameters(), lr=1e-3,weight_decay=1e-4)\nnum_epochs = 3\nfrom transformers import get_linear_schedule_with_warmup\n\n# Set up a scheduler with a warmup phase\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=500,  # Adjust the number of warmup steps\n    num_training_steps=num_epochs * len(train_dataset)\n)\n\n\n\nnum_training_steps = num_epochs * len(train_dataset)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n# Move model to GPU if available\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprefix_tuning_model.to(device)\n\n# Freeze the base model parameters and only train the prefix\nfor param in prefix_tuning_model.model.parameters():\n    param.requires_grad = False\n\n# Custom training loop\nfor epoch in range(num_epochs):\n    prefix_tuning_model.train()\n    total_loss = 0\n    \n    for idx in range(700):\n        batch = train_dataset[idx]\n\n        input_ids = torch.tensor(batch['input_ids']).unsqueeze(0).to(device)\n        labels = torch.tensor(batch['labels']).unsqueeze(0).to(device)\n\n        # Forward pass\n        outputs = prefix_tuning_model(input_ids=input_ids, labels=labels)\n        loss = outputs.loss\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n        print(f\"Epoch: {epoch + 1}/{num_epochs}, Step: {idx}, Loss: {loss.item()}\")\n\n    avg_loss = total_loss / len(train_dataset)\n    print(f\"Epoch {epoch + 1} finished. Average Loss: {avg_loss}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_inference(prefix_tuning_model, tokenizer, prompt, max_length=100, device='cuda'):\n    # Tokenize the input prompt\n    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n    \n    # Ensure correct padding and attention masks\n    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n    \n    # Set the model to evaluation mode\n    prefix_tuning_model.eval()\n\n    with torch.no_grad():  # No need to compute gradients during inference\n        # Extract model's embeddings for the input tokens\n        input_embeddings = prefix_tuning_model.model.get_input_embeddings()(input_ids)\n\n        # Generate prefix embeddings\n        prefix_embeddings = prefix_tuning_model.prefix_embedding.unsqueeze(0).expand(input_embeddings.size(0), -1, -1)\n\n        # Concatenate the prefix with the input embeddings\n        extended_embeddings = torch.cat((prefix_embeddings, input_embeddings), dim=1)\n\n        # Create an extended attention mask (including the prefix tokens)\n        prefix_attention_mask = torch.ones((input_embeddings.size(0), prefix_tuning_model.prefix_length), device=device)\n        extended_attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n\n        # Perform generation (inference)\n        generated_output = prefix_tuning_model.model.generate(\n            inputs_embeds=extended_embeddings,\n            attention_mask=extended_attention_mask,\n            max_length=max_length,\n            num_return_sequences=1,\n            no_repeat_ngram_size=2,  # Avoid repeating phrases\n            do_sample=False  # Greedy decoding\n        )\n\n        # Decode the generated tokens to text\n        output_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n    \n    return output_text\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example prompt\nprompt = \"If 2x+3=7, what is the value of x?\"\n\n# Running inference\noutput = run_inference(prefix_tuning_model, tokenizer, prompt, max_length=100, device='cuda')\n\n# Print the generated text\nprint(output)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}