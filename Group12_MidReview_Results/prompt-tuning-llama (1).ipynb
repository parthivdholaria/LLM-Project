{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install transformers datasets peft torch\n! pip install -U bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\nfrom datasets import load_dataset\nfrom peft import PromptTuningConfig, get_peft_model\n\n# Load the model and tokenizer with 4-bit quantization enabled and CPU offloading\nmodel_name = \"meta-llama/Llama-3.1-8B\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    use_auth_token=\"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\",\n    load_in_4bit=True,  # Enable 4-bit quantization\n    device_map=\"cuda\"  # Automatically map layers to available devices (CPU & GPU)  # Offload some layers to CPU in 32-bit precision\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_auth_token=\"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\"\n)\n\n# Define the Prompt Tuning configuration\npeft_config = PromptTuningConfig(\n    task_type=\"CAUSAL_LM\",    # Set the task type for causal language modeling\n    num_virtual_tokens=20     # Number of virtual tokens to tune (instead of prompt_length)\n)\n\n# Apply prompt tuning to the model\npeft_model = get_peft_model(model, peft_config)\n\n# Load the SVAMP dataset\ndataset = load_dataset(\"ChilleD/SVAMP\", split='train')\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess the dataset\n# Set the maximum length for tokenization\nMAX_LENGTH = 512  # Adjust this depending on the model's input size limit (common is 512 for many models)\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})  # or you can specify a custom pad token\n# Preprocess the dataset with truncation and padding to MAX_LENGTH\n# Set the maximum length for tokenization\nMAX_LENGTH = 512  # Adjust this depending on the model's input size limit\n\n# Preprocess function as defined previously\ndef preprocess_function(example):\n    inputs = example['Body'] + example['Question']\n    targets = example['Answer']\n    \n    # Tokenize the inputs and targets with max_length\n    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n    labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n    \n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Prepare the list to hold preprocessed data\npreprocessed_data = []\n\n# Loop through each sample in the dataset\nfor example in dataset:\n    # Apply the preprocess function to each sample\n    preprocessed_example = preprocess_function(example)\n    preprocessed_data.append(preprocessed_example)\n\n# Convert the list of preprocessed data to the required format for training\n\nprint(len(preprocessed_data))\nfrom datasets import Dataset\ntrain_dataset = Dataset.from_dict({key: [example[key] for example in preprocessed_data] for key in preprocessed_data[0].keys()})\n\n# Now `train_dataset` can be used in training\n\n\n\n# Define the optimizer and scheduler\noptimizer = AdamW(peft_model.parameters(), lr=5e-5)\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataset)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Move model to GPU if available, CPU offloading is handled by `device_map`\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\npeft_model.to(device)\n\n# Custom training loop\nfor epoch in range(num_epochs):\n    peft_model.train()  # Set the model to training mode\n    total_loss = 0\n    \n    for idx in range(700):\n        batch = train_dataset[idx]\n#         print(\"Step: \", step)\n#         print(\"\\n\")\n#         print(\"Batch: \", batch)\n        # Move inputs and labels to device\n        input_ids = torch.tensor(batch['input_ids']).unsqueeze(0).to(device)\n        labels = torch.tensor(batch['labels']).unsqueeze(0).to(device)\n\n        # Forward pass\n        outputs = peft_model(input_ids=input_ids, labels=labels)\n        loss = outputs.loss\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()  # Update learning rate\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n        # Print progress\n        print(f\"Epoch: {epoch + 1}/{num_epochs}, Step: {idx}, Loss: {loss.item()}\")\n\n    avg_loss = total_loss / len(train_dataset)\n    print(f\"Epoch {epoch + 1} finished. Average Loss: {avg_loss}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Function to perform inference on the fine-tuned model\n# def generate_response(prompt):\n#     # Tokenize the input prompt\n#     inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n    \n#     # Move the inputs to the appropriate device (CPU or GPU)\n#     input_ids = inputs['input_ids'].to(device)\n#     attention_mask = inputs['attention_mask'].to(device)\n    \n#     # Put the model in evaluation mode\n#     peft_model.eval()\n    \n#     # Generate a response using the model (greedy decoding for simplicity)\n#     with torch.no_grad():\n#         outputs = peft_model.generate(\n#             input_ids=input_ids,\n#             attention_mask=attention_mask,\n#             max_length=MAX_LENGTH,\n#             num_return_sequences=1,  # Generate only one sequence\n#             do_sample=False          # Greedy decoding\n#         )\n    \n#     # Decode the generated tokens back into text\n#     generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n#     return generated_text\n\n# # Sample prompt for testing (you can modify this with any other example)\n# sample_prompt = \"Solve the following equation: 2x + 3 = 7. What is the value of x?\"\n\n# # Generate response from the fine-tuned model\n# generated_output = generate_response(sample_prompt)\n\n# # Print the model's generated response\n# print(\"Prompt:\", sample_prompt)\n# print(\"Model's Response:\", generated_output)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the model to evaluation mode\npeft_model.eval()\n\nsample_prompt = \"Solve the equation: 2x + 3 = 7.\"\n\n# Tokenize the input prompt\ninputs = tokenizer.encode_plus(\n    sample_prompt, \n    return_tensors=\"pt\", \n    padding=True, \n    truncation=True,\n    max_length=512  # optional max_length\n).to(device)\n\n# Ensure attention mask and pad_token_id are set\ninput_ids = inputs[\"input_ids\"]\nattention_mask = inputs[\"attention_mask\"]\n\n# Generate the output with appropriate settings\ngenerated_output = peft_model.generate(\n    input_ids,\n    attention_mask=attention_mask,  # pass attention mask\n    max_length=512,  # Set the maximum number of tokens in the generated output\n    num_return_sequences=1,  # Generate one response\n    no_repeat_ngram_size=2,  # Optional: prevent repetition\n    do_sample=True,  # Use greedy decoding for deterministic output\n    pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n)\n\n# Decode and print the generated text\nprint(tokenizer.decode(generated_output[0], skip_special_tokens=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}