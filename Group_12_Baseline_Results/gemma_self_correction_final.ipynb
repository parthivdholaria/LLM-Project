{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "Requirement already satisfied: datasets in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (3.10.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: packaging in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install groq\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have become increasingly important in natural language processing (NLP) and machine learning in recent years, due to their ability to process and respond to large amounts of text quickly and accurately. Here are some reasons why fast language models are crucial:\n",
      "\n",
      "1. **Scalability:** Fast language models can handle large volumes of text data, making them ideal for applications where scalability is essential, such as chatbots, customer service, and social media analysis.\n",
      "\n",
      "2. **Real-time processing:** Fast language models enable real-time processing and response, allowing for applications like language translation, text summarization, and sentiment analysis to be performed instantaneously.\n",
      "\n",
      "3. **Improved user experience:** Fast language models can respond quickly to user queries, providing a seamless and interactive experience for users. For example, a language translation model can instantly translate text for real-time communication.\n",
      "\n",
      "4. **Handling unstructured data:** Fast language models can efficiently process unstructured data like text, emails, and comments, enabling applications like text classification, entity recognition, and topic modeling.\n",
      "\n",
      "5. **Time-sensitive applications:** Fast language models are critical for time-sensitive applications like emergency response systems, where immediate responses are crucial. They can quickly analyze and respond to emergency calls, messages, and social media reports.\n",
      "\n",
      "6. **Handling diverse languages:** Fast language models can be trained on diverse languages and dialects, enabling applications like multilingual chatbots, language learning platforms, and language translation services.\n",
      "\n",
      "7. **Reducing latency:** Fast language models can reduce latency in NLP applications, which is critical when dealing with high-stakes decisions, like sentiment analysis for financial trading or healthcare diagnosis.\n",
      "\n",
      "8. **Increased accuracy:** Fast language models can leverage large amounts of training data, enabling them to learn complex relationships and patterns in language, leading to increased accuracy in NLP applications.\n",
      "\n",
      "9. **Enabling autonomous systems:** Fast language models can be used to enable autonomous systems, like self-driving cars, drones, and robots, which rely on real-time language processing to make critical decisions.\n",
      "\n",
      "10. **Advancements in new AI applications:** Fast language models can pave the way for new AI applications, like AI-powered news articles, content generation, and creative writing, by enabling rapid processing and generation of high-volume text data.\n",
      "\n",
      "In summary, fast language models have revolutionized NLP and machine learning, enabling applications that require real-time processing, scalability, and accuracy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# from groq import Groq\n",
    "\n",
    "# client = Groq(\n",
    "#     api_key=\"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\",\n",
    "# )\n",
    "\n",
    "# chat_completion = client.chat.completions.create(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Explain the importance of fast language models\",\n",
    "#         }\n",
    "#     ],\n",
    "#     model=\"llama3-8b-8192\",\n",
    "# )\n",
    "\n",
    "# print(chat_completion.choices[0].message.content),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx:  0\n",
      "Idx:  1\n",
      "Idx:  2\n",
      "Idx:  3\n",
      "Idx:  4\n",
      "Idx:  5\n",
      "Idx:  6\n",
      "Idx:  7\n",
      "Idx:  8\n",
      "Idx:  9\n",
      "Idx:  10\n",
      "Idx:  11\n",
      "Idx:  12\n",
      "Idx:  13\n",
      "Idx:  14\n",
      "Idx:  15\n",
      "Idx:  16\n",
      "Idx:  17\n",
      "Idx:  18\n",
      "Idx:  19\n",
      "Idx:  20\n",
      "Idx:  21\n",
      "Idx:  22\n",
      "Idx:  23\n",
      "Idx:  24\n",
      "Idx:  25\n",
      "Idx:  26\n",
      "Idx:  27\n",
      "Idx:  28\n",
      "Idx:  29\n",
      "Idx:  30\n",
      "Idx:  31\n",
      "Idx:  32\n",
      "Idx:  33\n",
      "Idx:  34\n",
      "Idx:  35\n",
      "Idx:  36\n",
      "Idx:  37\n",
      "Idx:  38\n",
      "Idx:  39\n",
      "Idx:  40\n",
      "Idx:  41\n",
      "Idx:  42\n",
      "Idx:  43\n",
      "Idx:  44\n",
      "Idx:  45\n",
      "Idx:  46\n",
      "Idx:  47\n",
      "Idx:  48\n",
      "Idx:  49\n",
      "Idx:  50\n",
      "Idx:  51\n",
      "Idx:  52\n",
      "Idx:  53\n",
      "Idx:  54\n",
      "Idx:  55\n",
      "Idx:  56\n",
      "Idx:  57\n",
      "Idx:  58\n",
      "Idx:  59\n",
      "Idx:  60\n",
      "Idx:  61\n",
      "Idx:  62\n",
      "Idx:  63\n",
      "Idx:  64\n",
      "Idx:  65\n",
      "Idx:  66\n",
      "Idx:  67\n",
      "Idx:  68\n",
      "Idx:  69\n",
      "Idx:  70\n",
      "Idx:  71\n",
      "Idx:  72\n",
      "Idx:  73\n",
      "Idx:  74\n",
      "Idx:  75\n",
      "Idx:  76\n",
      "Idx:  77\n",
      "Idx:  78\n",
      "Idx:  79\n",
      "Idx:  80\n",
      "Idx:  81\n",
      "Idx:  82\n",
      "Idx:  83\n",
      "Idx:  84\n",
      "Idx:  85\n",
      "Idx:  86\n",
      "Idx:  87\n",
      "Idx:  88\n",
      "Idx:  89\n",
      "Idx:  90\n",
      "Idx:  91\n",
      "Idx:  92\n",
      "Idx:  93\n",
      "Idx:  94\n",
      "Idx:  95\n",
      "Idx:  96\n",
      "Idx:  97\n",
      "Idx:  98\n",
      "Idx:  99\n",
      "Idx:  100\n",
      "Idx:  101\n",
      "Idx:  102\n",
      "Idx:  103\n",
      "Idx:  104\n",
      "Idx:  105\n",
      "Idx:  106\n",
      "Idx:  107\n",
      "Idx:  108\n",
      "Idx:  109\n",
      "Idx:  110\n",
      "Idx:  111\n",
      "Idx:  112\n",
      "Idx:  113\n",
      "Idx:  114\n",
      "Idx:  115\n",
      "Idx:  116\n",
      "Idx:  117\n",
      "Idx:  118\n",
      "Idx:  119\n",
      "Idx:  120\n",
      "Idx:  121\n",
      "Idx:  122\n",
      "Idx:  123\n",
      "Idx:  124\n",
      "Idx:  125\n",
      "Idx:  126\n",
      "Idx:  127\n",
      "Idx:  128\n",
      "Idx:  129\n",
      "Idx:  130\n",
      "Idx:  131\n",
      "Idx:  132\n",
      "Idx:  133\n",
      "Idx:  134\n",
      "Idx:  135\n",
      "Idx:  136\n",
      "Idx:  137\n",
      "Idx:  138\n",
      "Idx:  139\n",
      "Idx:  140\n",
      "Idx:  141\n",
      "Idx:  142\n",
      "Idx:  143\n",
      "Idx:  144\n",
      "Idx:  145\n",
      "Idx:  146\n",
      "Idx:  147\n",
      "Idx:  148\n",
      "Idx:  149\n",
      "Idx:  150\n",
      "Idx:  151\n",
      "Idx:  152\n",
      "Idx:  153\n",
      "Idx:  154\n",
      "Idx:  155\n",
      "Idx:  156\n",
      "Idx:  157\n",
      "Idx:  158\n",
      "Idx:  159\n",
      "Idx:  160\n",
      "Idx:  161\n",
      "Idx:  162\n",
      "Idx:  163\n",
      "Idx:  164\n",
      "Idx:  165\n",
      "Idx:  166\n",
      "Idx:  167\n",
      "Idx:  168\n",
      "Idx:  169\n",
      "Idx:  170\n",
      "Idx:  171\n",
      "Idx:  172\n",
      "Idx:  173\n",
      "Idx:  174\n",
      "Idx:  175\n",
      "Idx:  176\n",
      "Idx:  177\n",
      "Idx:  178\n",
      "Idx:  179\n",
      "Idx:  180\n",
      "Idx:  181\n",
      "Idx:  182\n",
      "Idx:  183\n",
      "Idx:  184\n",
      "Idx:  185\n",
      "Idx:  186\n",
      "Idx:  187\n",
      "Idx:  188\n",
      "Idx:  189\n",
      "Idx:  190\n",
      "Idx:  191\n",
      "Idx:  192\n",
      "Idx:  193\n",
      "Idx:  194\n",
      "Idx:  195\n",
      "Idx:  196\n",
      "Idx:  197\n",
      "Idx:  198\n",
      "Idx:  199\n",
      "Idx:  200\n",
      "Idx:  201\n",
      "Idx:  202\n",
      "Idx:  203\n",
      "Idx:  204\n",
      "Idx:  205\n",
      "Idx:  206\n",
      "Idx:  207\n",
      "Idx:  208\n",
      "Idx:  209\n",
      "Idx:  210\n",
      "Idx:  211\n",
      "Idx:  212\n",
      "Idx:  213\n",
      "Idx:  214\n",
      "Idx:  215\n",
      "Idx:  216\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01j8wk53dnejab3ymjjsy4vnwk` on : Limit 500000, Used 500072, Requested 340. Please try again in 1m11.2232s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m correct_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(dataset[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# print(f\"\\nProblem {idx+1}: {problem}\")\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m response_1, response_2, response_3, initial_answer, final_answer \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_self_correction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# print(\"Response 1: \" + response_1)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(\"Response 2: \" + response_2)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# print(\"Response 3: \" + response_3)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# print(\"Initial Answer: \", initial_answer)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# print(\"Final Answer: \", final_answer)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m, in \u001b[0;36mevaluate_self_correction\u001b[0;34m(problem)\u001b[0m\n\u001b[1;32m     52\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_3})\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Prompt 3: Improve the answer based on the review\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m response_3 \u001b[38;5;241m=\u001b[39m \u001b[43mget_groq_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m get_groq_response([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_3\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mextraction_prompt}])\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(f\"Response 3: {response_3}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mget_groq_response\u001b[0;34m(chat_history)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_groq_response\u001b[39m(chat_history):\n\u001b[0;32m---> 26\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniforge3/envs/llm_env/lib/python3.10/site-packages/groq/resources/chat/completions.py:287\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm_env/lib/python3.10/site-packages/groq/_base_client.py:1244\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1241\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1242\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[0;32m-> 1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm_env/lib/python3.10/site-packages/groq/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm_env/lib/python3.10/site-packages/groq/_base_client.py:1039\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1038\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1042\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1043\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1048\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `gemma2-9b-it` in organization `org_01j8wk53dnejab3ymjjsy4vnwk` on : Limit 500000, Used 500072, Requested 340. Please try again in 1m11.2232s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(\n",
    "    api_key=\"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\"  # Ensure you have the Groq API Key set as an environment variable\n",
    ")\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "\n",
    "dataset = load_dataset(\"ChilleD/SVAMP\", split='test')\n",
    "\n",
    "# Define the model for Groq API calls\n",
    "model = \"gemma2-9b-it\"  # Change to the model you want to use\n",
    "\n",
    "# Prompt templates\n",
    "prompt_1 = \"Can you solve the following math problem? {problem} Explain your reasoning. Your final answer should be a single numerical number, in the form \\\\boxed{{answer}}, at the end of your response. Don't generate any additional text inside the \\\\boxed{{answer}}\"\n",
    "prompt_2 = \"Review your previous answer and find problems with your answer.\"\n",
    "prompt_3 = \"Based on the problems you found, improve your answer. Please reiterate your answer, with your final answer a single numerical number, in the form \\\\boxed{{answer}}. Don't generate any additional text inside the \\\\boxed{{answer}}\"\n",
    "\n",
    "extraction_prompt = \"Please extract the answer from the above text. Give the answer in the form \\\\boxed{{answer}} where {answer} is a single numerical number.\"\n",
    "\n",
    "# Function to make Groq API call\n",
    "def get_groq_response(chat_history):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=chat_history,\n",
    "        model=model,\n",
    "        temperature=0\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Evaluate self-correction on a sample of the dataset\n",
    "def evaluate_self_correction(problem):\n",
    "    # Prompt 1: Solve the problem\n",
    "    \n",
    "    chat_history = [{\"role\": \"user\", \"content\": prompt_1.format(problem=problem)}]\n",
    "\n",
    "    response_1 = get_groq_response(chat_history)\n",
    "    # print(f\"Response 1: {response_1}\")\n",
    "\n",
    "    initial_answer = get_groq_response([{\"role\": \"user\", \"content\": response_1+\"\\n\"+extraction_prompt}])\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_1})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt_2})\n",
    "\n",
    "    # Prompt 2: Review the previous answer\n",
    "    response_2 = get_groq_response(chat_history)\n",
    "    # print(f\"Response 2: {response_2}\")\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_2})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt_3})\n",
    "\n",
    "    # Prompt 3: Improve the answer based on the review\n",
    "    response_3 = get_groq_response(chat_history)\n",
    "\n",
    "    final_answer = get_groq_response([{\"role\": \"user\", \"content\": response_3+\"\\n\"+extraction_prompt}])\n",
    "    # print(f\"Response 3: {response_3}\")\n",
    "\n",
    "    return response_1, response_2, response_3, initial_answer, final_answer\n",
    "\n",
    "doubtful_responses = []\n",
    "\n",
    "\n",
    "\n",
    "def extract_answer(response):\n",
    "\n",
    "    start_index = response.find(\"\\\\boxed{\") + len(\"\\\\boxed{\")\n",
    "\n",
    "    end_index = response.find(\"}\", start_index)\n",
    "\n",
    "    answer = response[start_index:end_index]\n",
    "\n",
    "    # if(end_index == -1):\n",
    "    #     end_index = response.find(\"]\", start_index)\n",
    "    # if(end_index == -1):\n",
    "    #     end_index = response.find(\")\", start_index)\n",
    "\n",
    "    # print(\"Response: \", response)\n",
    "\n",
    "    # print(\"Start: \", start_index)\n",
    "    # print(\"End: \", end_index)\n",
    "\n",
    "    # print(\"Value: \"+response[start_index:end_index])\n",
    "\n",
    "    # answer = \"\"\n",
    "    # flag = False\n",
    "    # for i in range(start_index, end_index):\n",
    "    #     if response[i].isdigit():\n",
    "    #         flag = True\n",
    "    #         answer += response[i]\n",
    "    #     else:\n",
    "    #         if flag:\n",
    "    #             flag=False\n",
    "    #             break\n",
    "\n",
    "    return int(answer)\n",
    "\n",
    "# Evaluate on the first 5 problems in the dataset\n",
    "\n",
    "correct_1 = 0\n",
    "correct_2 = 0\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    print(\"Idx: \", idx)\n",
    "    problem = dataset[idx]['Body'] + \"\\n\" + dataset[idx]['Question']\n",
    "    correct_answer = int(dataset[idx]['Answer'])\n",
    "    # print(f\"\\nProblem {idx+1}: {problem}\")\n",
    "    response_1, response_2, response_3, initial_answer, final_answer = evaluate_self_correction(problem)\n",
    "\n",
    "    # print(\"Response 1: \" + response_1)\n",
    "    # print(\"Response 2: \" + response_2)\n",
    "    # print(\"Response 3: \" + response_3)\n",
    "    # print(\"Initial Answer: \", initial_answer)\n",
    "    # print(\"Final Answer: \", final_answer)\n",
    "\n",
    "    try:\n",
    "        initial_answer = extract_answer(initial_answer)\n",
    "    except:\n",
    "        doubtful_responses.append(response_1)\n",
    "        continue\n",
    "    # try:\n",
    "    #     answer_1 = int(answer_1.split(\" \")[0])\n",
    "    # except: \n",
    "    #     print(\"Response 1: \"+ response_1)\n",
    "    #     print(\"Answer 1: \"+ answer_1)\n",
    "    #     break\n",
    "    # answer_2 = extract_answer(response_2)\n",
    "    try:\n",
    "        final_answer = extract_answer(final_answer)\n",
    "    except: \n",
    "        doubtful_responses.append(response_3)\n",
    "        continue\n",
    "    # try:\n",
    "    #     answer_2 = int(answer_2.split(\" \")[0])\n",
    "    # except: \n",
    "    #     print(\"Response 2: \"+ response_3)\n",
    "    #     print(\"Answer 2: \"+ answer_2)\n",
    "    #     break\n",
    "    # print(\"Initial Answer: \", initial_answer)\n",
    "    # print(\"Final Answer: \", final_answer)\n",
    "\n",
    "    # print(\"Initial_answer: \", initial_answer)\n",
    "    # print(\"Final_answer: \", final_answer)\n",
    "    # print(\"Correct_answer: \", correct_answer)\n",
    "    \n",
    "    if(initial_answer == correct_answer):\n",
    "        correct_1 += 1\n",
    "    if(final_answer == correct_answer):\n",
    "        correct_2 += 1\n",
    "\n",
    "\n",
    "accuracy_1 = correct_1/(300-len(doubtful_responses)) * 100\n",
    "accuracy_2 = correct_2/(300-len(doubtful_responses)) * 100\n",
    "\n",
    "print(\"SVAMP Dataset: \")\n",
    "print(\"Accuracy before self-correction: \", accuracy_1)\n",
    "print(\"Accuracy after self-correction: \", accuracy_2)\n",
    "\n",
    "print(\"Incorrect Format: \", len(doubtful_responses))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self Correction for Gemma 7B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVAMP Dataset: \n",
      "Accuracy before self-correction:  90.42553191489363\n",
      "Accuracy after self-correction:  85.63829787234043\n",
      "Incorrect Format:  27\n"
     ]
    }
   ],
   "source": [
    "accuracy_1 = correct_1/(215-len(doubtful_responses)) * 100\n",
    "accuracy_2 = correct_2/(215-len(doubtful_responses)) * 100\n",
    "\n",
    "print(\"SVAMP Dataset: \")\n",
    "print(\"Accuracy before self-correction: \", accuracy_1)\n",
    "print(\"Accuracy after self-correction: \", accuracy_2)\n",
    "\n",
    "print(\"Incorrect Format: \", len(doubtful_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"3319921\".isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "a = '145'\n",
    "b = int(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx:  0\n",
      "Idx:  1\n",
      "Idx:  2\n",
      "Idx:  3\n",
      "Idx:  4\n",
      "Idx:  5\n",
      "Idx:  6\n",
      "Idx:  7\n",
      "Idx:  8\n",
      "Idx:  9\n",
      "Idx:  10\n",
      "Idx:  11\n",
      "Idx:  12\n",
      "Idx:  13\n",
      "Idx:  14\n",
      "Idx:  15\n",
      "Idx:  16\n",
      "Idx:  17\n",
      "Idx:  18\n",
      "Idx:  19\n",
      "Idx:  20\n",
      "Idx:  21\n",
      "Idx:  22\n",
      "Idx:  23\n",
      "Idx:  24\n",
      "Idx:  25\n",
      "Idx:  26\n",
      "Idx:  27\n",
      "Idx:  28\n",
      "Idx:  29\n",
      "Idx:  30\n",
      "Idx:  31\n",
      "Idx:  32\n",
      "Idx:  33\n",
      "Idx:  34\n",
      "Idx:  35\n",
      "Idx:  36\n",
      "Idx:  37\n",
      "Idx:  38\n",
      "Idx:  39\n",
      "Idx:  40\n",
      "Idx:  41\n",
      "Idx:  42\n",
      "Idx:  43\n",
      "Idx:  44\n",
      "Idx:  45\n",
      "Idx:  46\n",
      "Idx:  47\n",
      "Idx:  48\n",
      "Idx:  49\n",
      "Idx:  50\n",
      "Idx:  51\n",
      "Idx:  52\n",
      "Idx:  53\n",
      "Idx:  54\n",
      "Idx:  55\n",
      "Idx:  56\n",
      "Idx:  57\n",
      "Idx:  58\n",
      "Idx:  59\n",
      "Idx:  60\n",
      "Idx:  61\n",
      "Idx:  62\n",
      "Idx:  63\n",
      "Idx:  64\n",
      "Idx:  65\n",
      "Idx:  66\n",
      "Idx:  67\n",
      "Idx:  68\n",
      "Idx:  69\n",
      "Idx:  70\n",
      "Idx:  71\n",
      "Idx:  72\n",
      "Idx:  73\n",
      "Idx:  74\n",
      "Idx:  75\n",
      "Idx:  76\n",
      "Idx:  77\n",
      "Idx:  78\n",
      "Idx:  79\n",
      "Idx:  80\n",
      "Idx:  81\n",
      "Idx:  82\n",
      "Idx:  83\n",
      "Idx:  84\n",
      "Idx:  85\n",
      "Idx:  86\n",
      "Idx:  87\n",
      "Idx:  88\n",
      "Idx:  89\n",
      "Idx:  90\n",
      "Idx:  91\n",
      "Idx:  92\n",
      "Idx:  93\n",
      "Idx:  94\n",
      "Idx:  95\n",
      "Idx:  96\n",
      "Idx:  97\n",
      "Idx:  98\n",
      "Idx:  99\n",
      "GSM8K Dataset: \n",
      "Accuracy before self-correction:  85.71428571428571\n",
      "Accuracy after self-correction:  83.51648351648352\n",
      "Incorrect Format:  9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(\n",
    "    api_key=\"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\"  # Ensure you have the Groq API Key set as an environment variable\n",
    ")\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "# Define the model for Groq API calls\n",
    "model = \"gemma2-9b-it\"  # Change to the model you want to use\n",
    "\n",
    "# Prompt templates\n",
    "prompt_1 = \"Can you solve the following math problem? {problem} Explain your reasoning. Your final answer should be a single numerical number, in the form \\\\boxed{{answer}}, at the end of your response. Don't generate any additional text inside the \\\\boxed{{answer}}\"\n",
    "prompt_2 = \"Review your previous answer and find problems with your answer.\"\n",
    "prompt_3 = \"Based on the problems you found, improve your answer. Please reiterate your answer, with your final answer a single numerical number, in the form \\\\boxed{{answer}}. Don't generate any additional text inside the \\\\boxed{{answer}}\"\n",
    "\n",
    "extraction_prompt = \"Please extract the answer from the above text. Give the answer in the form \\\\boxed{{answer}} where {answer} is a single numerical number.\"\n",
    "\n",
    "# Function to make Groq API call\n",
    "def get_groq_response(chat_history):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=chat_history,\n",
    "        model=model,\n",
    "        temperature=0\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Evaluate self-correction on a sample of the dataset\n",
    "def evaluate_self_correction(problem):\n",
    "    # Prompt 1: Solve the problem\n",
    "    \n",
    "    chat_history = [{\"role\": \"user\", \"content\": prompt_1.format(problem=problem)}]\n",
    "\n",
    "    response_1 = get_groq_response(chat_history)\n",
    "    # print(f\"Response 1: {response_1}\")\n",
    "\n",
    "    initial_answer = get_groq_response([{\"role\": \"user\", \"content\": response_1+\"\\n\"+extraction_prompt}])\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_1})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt_2})\n",
    "\n",
    "    # Prompt 2: Review the previous answer\n",
    "    response_2 = get_groq_response(chat_history)\n",
    "    # print(f\"Response 2: {response_2}\")\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_2})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt_3})\n",
    "\n",
    "    # Prompt 3: Improve the answer based on the review\n",
    "    response_3 = get_groq_response(chat_history)\n",
    "\n",
    "    final_answer = get_groq_response([{\"role\": \"user\", \"content\": response_3+\"\\n\"+extraction_prompt}])\n",
    "    # print(f\"Response 3: {response_3}\")\n",
    "\n",
    "    return response_1, response_2, response_3, initial_answer, final_answer\n",
    "\n",
    "doubtful_responses = []\n",
    "\n",
    "def extract_correct_answer(answer_string):\n",
    "    start_index = answer_string.find(\"####\") + len(\"#### \")\n",
    "    end_index = len(answer_string)\n",
    "\n",
    "    answer = answer_string[start_index:end_index]\n",
    "\n",
    "    return int(answer)\n",
    "\n",
    "\n",
    "\n",
    "def extract_answer(response):\n",
    "\n",
    "    start_index = response.find(\"\\\\boxed{\") + len(\"\\\\boxed{\")\n",
    "\n",
    "    end_index = response.find(\"}\", start_index)\n",
    "\n",
    "    answer = response[start_index:end_index]\n",
    "\n",
    "    # if(end_index == -1):\n",
    "    #     end_index = response.find(\"]\", start_index)\n",
    "    # if(end_index == -1):\n",
    "    #     end_index = response.find(\")\", start_index)\n",
    "\n",
    "    # print(\"Response: \", response)\n",
    "\n",
    "    # print(\"Start: \", start_index)\n",
    "    # print(\"End: \", end_index)\n",
    "\n",
    "    # print(\"Value: \"+response[start_index:end_index])\n",
    "\n",
    "    # answer = \"\"\n",
    "    # flag = False\n",
    "    # for i in range(start_index, end_index):\n",
    "    #     if response[i].isdigit():\n",
    "    #         flag = True\n",
    "    #         answer += response[i]\n",
    "    #     else:\n",
    "    #         if flag:\n",
    "    #             flag=False\n",
    "    #             break\n",
    "\n",
    "    return int(answer)\n",
    "\n",
    "# Evaluate on the first 5 problems in the dataset\n",
    "\n",
    "correct_1 = 0\n",
    "correct_2 = 0\n",
    "\n",
    "for idx in range(100):\n",
    "    print(\"Idx: \", idx)\n",
    "    problem = ds[idx]['question']\n",
    "    correct_answer = extract_correct_answer(ds[idx]['answer'])\n",
    "    # print(f\"\\nProblem {idx+1}: {problem}\")\n",
    "    response_1, response_2, response_3, initial_answer, final_answer = evaluate_self_correction(problem)\n",
    "\n",
    "    # print(\"Response 1: \" + response_1)\n",
    "    # print(\"Response 2: \" + response_2)\n",
    "    # print(\"Response 3: \" + response_3)\n",
    "    # print(\"Initial Answer: \", initial_answer)\n",
    "    # print(\"Final Answer: \", final_answer)\n",
    "\n",
    "    try:\n",
    "        initial_answer = extract_answer(initial_answer)\n",
    "    except:\n",
    "        doubtful_responses.append(response_1)\n",
    "        continue\n",
    "    # try:\n",
    "    #     answer_1 = int(answer_1.split(\" \")[0])\n",
    "    # except: \n",
    "    #     print(\"Response 1: \"+ response_1)\n",
    "    #     print(\"Answer 1: \"+ answer_1)\n",
    "    #     break\n",
    "    # answer_2 = extract_answer(response_2)\n",
    "    try:\n",
    "        final_answer = extract_answer(final_answer)\n",
    "    except: \n",
    "        doubtful_responses.append(response_3)\n",
    "        continue\n",
    "    # try:\n",
    "    #     answer_2 = int(answer_2.split(\" \")[0])\n",
    "    # except: \n",
    "    #     print(\"Response 2: \"+ response_3)\n",
    "    #     print(\"Answer 2: \"+ answer_2)\n",
    "    #     break\n",
    "    # print(\"Initial Answer: \", initial_answer)\n",
    "    # print(\"Final Answer: \", final_answer)\n",
    "\n",
    "    # print(\"Initial_answer: \", initial_answer)\n",
    "    # print(\"Final_answer: \", final_answer)\n",
    "    # print(\"Correct_answer: \", correct_answer)\n",
    "    \n",
    "    if(initial_answer == correct_answer):\n",
    "        correct_1 += 1\n",
    "    if(final_answer == correct_answer):\n",
    "        correct_2 += 1\n",
    "\n",
    "\n",
    "accuracy_1 = correct_1/(100-len(doubtful_responses)) * 100\n",
    "accuracy_2 = correct_2/(100-len(doubtful_responses)) * 100\n",
    "\n",
    "print(\"GSM8K Dataset: \")\n",
    "print(\"Accuracy before self-correction: \", accuracy_1)\n",
    "print(\"Accuracy after self-correction: \", accuracy_2)\n",
    "\n",
    "print(\"Incorrect Format: \", len(doubtful_responses))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
