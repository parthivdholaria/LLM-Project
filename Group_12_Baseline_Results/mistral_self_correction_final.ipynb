{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "Requirement already satisfied: datasets in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (3.10.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: packaging in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aayush/miniforge3/envs/llm_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install groq\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have become increasingly important in natural language processing (NLP) and machine learning in recent years, due to their ability to process and respond to large amounts of text quickly and accurately. Here are some reasons why fast language models are crucial:\n",
      "\n",
      "1. **Scalability:** Fast language models can handle large volumes of text data, making them ideal for applications where scalability is essential, such as chatbots, customer service, and social media analysis.\n",
      "\n",
      "2. **Real-time processing:** Fast language models enable real-time processing and response, allowing for applications like language translation, text summarization, and sentiment analysis to be performed instantaneously.\n",
      "\n",
      "3. **Improved user experience:** Fast language models can respond quickly to user queries, providing a seamless and interactive experience for users. For example, a language translation model can instantly translate text for real-time communication.\n",
      "\n",
      "4. **Handling unstructured data:** Fast language models can efficiently process unstructured data like text, emails, and comments, enabling applications like text classification, entity recognition, and topic modeling.\n",
      "\n",
      "5. **Time-sensitive applications:** Fast language models are critical for time-sensitive applications like emergency response systems, where immediate responses are crucial. They can quickly analyze and respond to emergency calls, messages, and social media reports.\n",
      "\n",
      "6. **Handling diverse languages:** Fast language models can be trained on diverse languages and dialects, enabling applications like multilingual chatbots, language learning platforms, and language translation services.\n",
      "\n",
      "7. **Reducing latency:** Fast language models can reduce latency in NLP applications, which is critical when dealing with high-stakes decisions, like sentiment analysis for financial trading or healthcare diagnosis.\n",
      "\n",
      "8. **Increased accuracy:** Fast language models can leverage large amounts of training data, enabling them to learn complex relationships and patterns in language, leading to increased accuracy in NLP applications.\n",
      "\n",
      "9. **Enabling autonomous systems:** Fast language models can be used to enable autonomous systems, like self-driving cars, drones, and robots, which rely on real-time language processing to make critical decisions.\n",
      "\n",
      "10. **Advancements in new AI applications:** Fast language models can pave the way for new AI applications, like AI-powered news articles, content generation, and creative writing, by enabling rapid processing and generation of high-volume text data.\n",
      "\n",
      "In summary, fast language models have revolutionized NLP and machine learning, enabling applications that require real-time processing, scalability, and accuracy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# from groq import Groq\n",
    "\n",
    "# client = Groq(\n",
    "#     api_key=\"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\",\n",
    "# )\n",
    "\n",
    "# chat_completion = client.chat.completions.create(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Explain the importance of fast language models\",\n",
    "#         }\n",
    "#     ],\n",
    "#     model=\"llama3-8b-8192\",\n",
    "# )\n",
    "\n",
    "# print(chat_completion.choices[0].message.content),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self Correction for Gemma 7B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx:  0\n",
      "Idx:  1\n",
      "Idx:  2\n",
      "Idx:  3\n",
      "Idx:  4\n",
      "Idx:  5\n",
      "Idx:  6\n",
      "Idx:  7\n",
      "Idx:  8\n",
      "Idx:  9\n",
      "Idx:  10\n",
      "Idx:  11\n",
      "Idx:  12\n",
      "Idx:  13\n",
      "Idx:  14\n",
      "Idx:  15\n",
      "Idx:  16\n",
      "Idx:  17\n",
      "Idx:  18\n",
      "Idx:  19\n",
      "Idx:  20\n",
      "Idx:  21\n",
      "Idx:  22\n",
      "Idx:  23\n",
      "Idx:  24\n",
      "Idx:  25\n",
      "Idx:  26\n",
      "Idx:  27\n",
      "Idx:  28\n",
      "Idx:  29\n",
      "Idx:  30\n",
      "Idx:  31\n",
      "Idx:  32\n",
      "Idx:  33\n",
      "Idx:  34\n",
      "Idx:  35\n",
      "Idx:  36\n",
      "Idx:  37\n",
      "Idx:  38\n",
      "Idx:  39\n",
      "Idx:  40\n",
      "Idx:  41\n",
      "Idx:  42\n",
      "Idx:  43\n",
      "Idx:  44\n",
      "Idx:  45\n",
      "Idx:  46\n",
      "Idx:  47\n",
      "Idx:  48\n",
      "Idx:  49\n",
      "Idx:  50\n",
      "Idx:  51\n",
      "Idx:  52\n",
      "Idx:  53\n",
      "Idx:  54\n",
      "Idx:  55\n",
      "Idx:  56\n",
      "Idx:  57\n",
      "Idx:  58\n",
      "Idx:  59\n",
      "Idx:  60\n",
      "Idx:  61\n",
      "Idx:  62\n",
      "Idx:  63\n",
      "Idx:  64\n",
      "Idx:  65\n",
      "Idx:  66\n",
      "Idx:  67\n",
      "Idx:  68\n",
      "Idx:  69\n",
      "Idx:  70\n",
      "Idx:  71\n",
      "Idx:  72\n",
      "Idx:  73\n",
      "Idx:  74\n",
      "Idx:  75\n",
      "Idx:  76\n",
      "Idx:  77\n",
      "Idx:  78\n",
      "Idx:  79\n",
      "Idx:  80\n",
      "Idx:  81\n",
      "Idx:  82\n",
      "Idx:  83\n",
      "Idx:  84\n",
      "Idx:  85\n",
      "Idx:  86\n",
      "Idx:  87\n",
      "Idx:  88\n",
      "Idx:  89\n",
      "Idx:  90\n",
      "Idx:  91\n",
      "Idx:  92\n",
      "Idx:  93\n",
      "Idx:  94\n",
      "Idx:  95\n",
      "Idx:  96\n",
      "Idx:  97\n",
      "Idx:  98\n",
      "Idx:  99\n",
      "Idx:  100\n",
      "Idx:  101\n",
      "Idx:  102\n",
      "Idx:  103\n",
      "Idx:  104\n",
      "Idx:  105\n",
      "Idx:  106\n",
      "Idx:  107\n",
      "Idx:  108\n",
      "Idx:  109\n",
      "Idx:  110\n",
      "Idx:  111\n",
      "Idx:  112\n",
      "Idx:  113\n",
      "Idx:  114\n",
      "Idx:  115\n",
      "Idx:  116\n",
      "Idx:  117\n",
      "Idx:  118\n",
      "Idx:  119\n",
      "Idx:  120\n",
      "Idx:  121\n",
      "Idx:  122\n",
      "Idx:  123\n",
      "Idx:  124\n",
      "Idx:  125\n",
      "Idx:  126\n",
      "Idx:  127\n",
      "Idx:  128\n",
      "Idx:  129\n",
      "Idx:  130\n",
      "Idx:  131\n",
      "Idx:  132\n",
      "Idx:  133\n",
      "Idx:  134\n",
      "Idx:  135\n",
      "Idx:  136\n",
      "Idx:  137\n",
      "Idx:  138\n",
      "Idx:  139\n",
      "Idx:  140\n",
      "Idx:  141\n",
      "Idx:  142\n",
      "Idx:  143\n",
      "Idx:  144\n",
      "Idx:  145\n",
      "Idx:  146\n",
      "Idx:  147\n",
      "Idx:  148\n",
      "Idx:  149\n",
      "Idx:  150\n",
      "Idx:  151\n",
      "Idx:  152\n",
      "Idx:  153\n",
      "Idx:  154\n",
      "Idx:  155\n",
      "Idx:  156\n",
      "Idx:  157\n",
      "Idx:  158\n",
      "Idx:  159\n",
      "Idx:  160\n",
      "Idx:  161\n",
      "Idx:  162\n",
      "Idx:  163\n",
      "Idx:  164\n",
      "Idx:  165\n",
      "Idx:  166\n",
      "Idx:  167\n",
      "Idx:  168\n",
      "Idx:  169\n",
      "Idx:  170\n",
      "Idx:  171\n",
      "Idx:  172\n",
      "Idx:  173\n",
      "Idx:  174\n",
      "Idx:  175\n",
      "Idx:  176\n",
      "Idx:  177\n",
      "Idx:  178\n",
      "Idx:  179\n",
      "Idx:  180\n",
      "Idx:  181\n",
      "Idx:  182\n",
      "Idx:  183\n",
      "Idx:  184\n",
      "Idx:  185\n",
      "Idx:  186\n",
      "Idx:  187\n",
      "Idx:  188\n",
      "Idx:  189\n",
      "Idx:  190\n",
      "Idx:  191\n",
      "Idx:  192\n",
      "Idx:  193\n",
      "Idx:  194\n",
      "Idx:  195\n",
      "Idx:  196\n",
      "Idx:  197\n",
      "Idx:  198\n",
      "Idx:  199\n",
      "Idx:  200\n",
      "Idx:  201\n",
      "Idx:  202\n",
      "Idx:  203\n",
      "Idx:  204\n",
      "Idx:  205\n",
      "Idx:  206\n",
      "Idx:  207\n",
      "Idx:  208\n",
      "Idx:  209\n",
      "Idx:  210\n",
      "Idx:  211\n",
      "Idx:  212\n",
      "Idx:  213\n",
      "Idx:  214\n",
      "Idx:  215\n",
      "Idx:  216\n",
      "Idx:  217\n",
      "Idx:  218\n",
      "Idx:  219\n",
      "Idx:  220\n",
      "Idx:  221\n",
      "Idx:  222\n",
      "Idx:  223\n",
      "Idx:  224\n",
      "Idx:  225\n",
      "Idx:  226\n",
      "Idx:  227\n",
      "Idx:  228\n",
      "Idx:  229\n",
      "Idx:  230\n",
      "Idx:  231\n",
      "Idx:  232\n",
      "Idx:  233\n",
      "Idx:  234\n",
      "Idx:  235\n",
      "Idx:  236\n",
      "Idx:  237\n",
      "Idx:  238\n",
      "Idx:  239\n",
      "Idx:  240\n",
      "Idx:  241\n",
      "Idx:  242\n",
      "Idx:  243\n",
      "Idx:  244\n",
      "Idx:  245\n",
      "Idx:  246\n",
      "Idx:  247\n",
      "Idx:  248\n",
      "Idx:  249\n",
      "Idx:  250\n",
      "Idx:  251\n",
      "Idx:  252\n",
      "Idx:  253\n",
      "Idx:  254\n",
      "Idx:  255\n",
      "Idx:  256\n",
      "Idx:  257\n",
      "Idx:  258\n",
      "Idx:  259\n",
      "Idx:  260\n",
      "Idx:  261\n",
      "Idx:  262\n",
      "Idx:  263\n",
      "Idx:  264\n",
      "Idx:  265\n",
      "Idx:  266\n",
      "Idx:  267\n",
      "Idx:  268\n",
      "Idx:  269\n",
      "Idx:  270\n",
      "Idx:  271\n",
      "Idx:  272\n",
      "Idx:  273\n",
      "Idx:  274\n",
      "Idx:  275\n",
      "Idx:  276\n",
      "Idx:  277\n",
      "Idx:  278\n",
      "Idx:  279\n",
      "Idx:  280\n",
      "Idx:  281\n",
      "Idx:  282\n",
      "Idx:  283\n",
      "Idx:  284\n",
      "Idx:  285\n",
      "Idx:  286\n",
      "Idx:  287\n",
      "Idx:  288\n",
      "Idx:  289\n",
      "Idx:  290\n",
      "Idx:  291\n",
      "Idx:  292\n",
      "Idx:  293\n",
      "Idx:  294\n",
      "Idx:  295\n",
      "Idx:  296\n",
      "Idx:  297\n",
      "Idx:  298\n",
      "Idx:  299\n",
      "Accuracy before self-correction:  195\n",
      "Accuracy after self-correction:  183\n",
      "Incorrect Format:  49\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(\n",
    "    api_key=\"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\"  # Ensure you have the Groq API Key set as an environment variable\n",
    ")\n",
    "\n",
    "# Load the SVAMP dataset\n",
    "dataset = load_dataset(\"ChilleD/SVAMP\", split='test')\n",
    "\n",
    "# Define the model for Groq API calls\n",
    "model = \"mixtral-8x7b-32768\"  # Change to the model you want to use\n",
    "\n",
    "# Prompt templates\n",
    "prompt_1 = \"Can you solve the following math problem? {problem} Explain your reasoning. Your final answer should be a single numerical number, in the form \\\\boxed{{answer}}, at the end of your response. Don't generate any additional text inside the \\\\boxed{{answer}}\"\n",
    "prompt_2 = \"Review your previous answer and find problems with your answer.\"\n",
    "prompt_3 = \"Based on the problems you found, improve your answer. Please reiterate your answer, with your final answer a single numerical number, in the form \\\\boxed{{answer}}. Don't generate any additional text inside the \\\\boxed{{answer}}\"\n",
    "\n",
    "extraction_prompt = \"Please extract the answer from the above text. Give the answer in the form \\\\boxed{{answer}} where {answer} is a single numerical number.\"\n",
    "\n",
    "# Function to make Groq API call\n",
    "def get_groq_response(chat_history):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=chat_history,\n",
    "        model=model,\n",
    "        temperature=0\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Evaluate self-correction on a sample of the dataset\n",
    "def evaluate_self_correction(problem):\n",
    "    # Prompt 1: Solve the problem\n",
    "    \n",
    "    chat_history = [{\"role\": \"user\", \"content\": prompt_1.format(problem=problem)}]\n",
    "\n",
    "    response_1 = get_groq_response(chat_history)\n",
    "    # print(f\"Response 1: {response_1}\")\n",
    "\n",
    "    initial_answer = get_groq_response([{\"role\": \"user\", \"content\": response_1+\"\\n\"+extraction_prompt}])\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_1})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt_2})\n",
    "\n",
    "    # Prompt 2: Review the previous answer\n",
    "    response_2 = get_groq_response(chat_history)\n",
    "    # print(f\"Response 2: {response_2}\")\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_2})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt_3})\n",
    "\n",
    "    # Prompt 3: Improve the answer based on the review\n",
    "    response_3 = get_groq_response(chat_history)\n",
    "\n",
    "    final_answer = get_groq_response([{\"role\": \"user\", \"content\": response_3+\"\\n\"+extraction_prompt}])\n",
    "    # print(f\"Response 3: {response_3}\")\n",
    "\n",
    "    return response_1, response_2, response_3, initial_answer, final_answer\n",
    "\n",
    "doubtful_responses = []\n",
    "\n",
    "def extract_answer(response):\n",
    "\n",
    "    start_index = response.find(\"\\\\boxed{\") + len(\"\\\\boxed{\")\n",
    "\n",
    "    end_index = response.find(\"}\", start_index)\n",
    "\n",
    "    answer = response[start_index:end_index]\n",
    "\n",
    "    # if(end_index == -1):\n",
    "    #     end_index = response.find(\"]\", start_index)\n",
    "    # if(end_index == -1):\n",
    "    #     end_index = response.find(\")\", start_index)\n",
    "\n",
    "    # print(\"Response: \", response)\n",
    "\n",
    "    # print(\"Start: \", start_index)\n",
    "    # print(\"End: \", end_index)\n",
    "\n",
    "    # print(\"Value: \"+response[start_index:end_index])\n",
    "\n",
    "    # answer = \"\"\n",
    "    # flag = False\n",
    "    # for i in range(start_index, end_index):\n",
    "    #     if response[i].isdigit():\n",
    "    #         flag = True\n",
    "    #         answer += response[i]\n",
    "    #     else:\n",
    "    #         if flag:\n",
    "    #             flag=False\n",
    "    #             break\n",
    "\n",
    "    return int(answer)\n",
    "\n",
    "# Evaluate on the first 5 problems in the dataset\n",
    "\n",
    "correct_1 = 0\n",
    "correct_2 = 0\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    print(\"Idx: \", idx)\n",
    "    problem = dataset[idx]['Body'] + \" \" + dataset[idx]['Question']\n",
    "    correct_answer = int(dataset[idx]['Answer'])\n",
    "    # print(f\"\\nProblem {idx+1}: {problem}\")\n",
    "    response_1, response_2, response_3, initial_answer, final_answer = evaluate_self_correction(problem)\n",
    "\n",
    "    try:\n",
    "        initial_answer = extract_answer(initial_answer)\n",
    "    except:\n",
    "        doubtful_responses.append(response_1)\n",
    "        continue\n",
    "    # try:\n",
    "    #     answer_1 = int(answer_1.split(\" \")[0])\n",
    "    # except: \n",
    "    #     print(\"Response 1: \"+ response_1)\n",
    "    #     print(\"Answer 1: \"+ answer_1)\n",
    "    #     break\n",
    "    # answer_2 = extract_answer(response_2)\n",
    "    try:\n",
    "        final_answer = extract_answer(final_answer)\n",
    "    except: \n",
    "        doubtful_responses.append(response_3)\n",
    "        continue\n",
    "    # try:\n",
    "    #     answer_2 = int(answer_2.split(\" \")[0])\n",
    "    # except: \n",
    "    #     print(\"Response 2: \"+ response_3)\n",
    "    #     print(\"Answer 2: \"+ answer_2)\n",
    "    #     break\n",
    "    # print(\"Initial Answer: \", initial_answer)\n",
    "    # print(\"Final Answer: \", final_answer)\n",
    "    \n",
    "    if(initial_answer == correct_answer):\n",
    "        correct_1 += 1\n",
    "    if(final_answer == correct_answer):\n",
    "        correct_2 += 1\n",
    "\n",
    "\n",
    "accuracy_1 = correct_1/(100-len(doubtful_responses)) * 100\n",
    "accuracy_2 = correct_2/(100-len(doubtful_responses)) * 100\n",
    "\n",
    "print(\"Accuracy before self-correction: \", correct_1)\n",
    "print(\"Accuracy after self-correction: \", correct_2)\n",
    "\n",
    "print(\"Incorrect Format: \", len(doubtful_responses))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before self-correction:  77.68924302788844\n",
      "Accuracy after self-correction:  72.90836653386454\n",
      "Incorrect Format:  49\n"
     ]
    }
   ],
   "source": [
    "accuracy_1 = correct_1/(len(dataset)-len(doubtful_responses)) * 100\n",
    "accuracy_2 = correct_2/(len(dataset)-len(doubtful_responses)) * 100\n",
    "\n",
    "print(\"Accuracy before self-correction: \", accuracy_1)\n",
    "print(\"Accuracy after self-correction: \", accuracy_2)\n",
    "\n",
    "print(\"Incorrect Format: \", len(doubtful_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"3319921\".isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "a = '145'\n",
    "b = int(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx:  0\n",
      "Idx:  1\n",
      "Idx:  2\n",
      "Idx:  3\n",
      "Idx:  4\n",
      "Idx:  5\n",
      "Idx:  6\n",
      "Idx:  7\n",
      "Idx:  8\n",
      "Idx:  9\n",
      "Idx:  10\n",
      "Idx:  11\n",
      "Idx:  12\n",
      "Idx:  13\n",
      "Idx:  14\n",
      "Idx:  15\n",
      "Idx:  16\n",
      "Idx:  17\n",
      "Idx:  18\n",
      "Idx:  19\n",
      "Idx:  20\n",
      "Idx:  21\n",
      "Idx:  22\n",
      "Idx:  23\n",
      "Idx:  24\n",
      "Idx:  25\n",
      "Idx:  26\n",
      "Idx:  27\n",
      "Idx:  28\n",
      "Idx:  29\n",
      "Idx:  30\n",
      "Idx:  31\n",
      "Idx:  32\n",
      "Idx:  33\n",
      "Idx:  34\n",
      "Idx:  35\n",
      "Idx:  36\n",
      "Idx:  37\n",
      "Idx:  38\n",
      "Idx:  39\n",
      "Idx:  40\n",
      "Idx:  41\n",
      "Idx:  42\n",
      "Idx:  43\n",
      "Idx:  44\n",
      "Idx:  45\n",
      "Idx:  46\n",
      "Idx:  47\n",
      "Idx:  48\n",
      "Idx:  49\n",
      "Idx:  50\n",
      "Idx:  51\n",
      "Idx:  52\n",
      "Idx:  53\n",
      "Idx:  54\n",
      "Idx:  55\n",
      "Idx:  56\n",
      "Idx:  57\n",
      "Idx:  58\n",
      "Idx:  59\n",
      "Idx:  60\n",
      "Idx:  61\n",
      "Idx:  62\n",
      "Idx:  63\n",
      "Idx:  64\n",
      "Idx:  65\n",
      "Idx:  66\n",
      "Idx:  67\n",
      "Idx:  68\n",
      "Idx:  69\n",
      "Idx:  70\n",
      "Idx:  71\n",
      "Idx:  72\n",
      "Idx:  73\n",
      "Idx:  74\n",
      "Idx:  75\n",
      "Idx:  76\n",
      "Idx:  77\n",
      "Idx:  78\n",
      "Idx:  79\n",
      "Idx:  80\n",
      "Idx:  81\n",
      "Idx:  82\n",
      "Idx:  83\n",
      "Idx:  84\n",
      "Idx:  85\n",
      "Idx:  86\n",
      "Idx:  87\n",
      "Idx:  88\n",
      "Idx:  89\n",
      "Idx:  90\n",
      "Idx:  91\n",
      "Idx:  92\n",
      "Idx:  93\n",
      "Idx:  94\n",
      "Idx:  95\n",
      "Idx:  96\n",
      "Idx:  97\n",
      "Idx:  98\n",
      "Idx:  99\n",
      "GSM8K Dataset: \n",
      "Accuracy before self-correction:  77.01149425287356\n",
      "Accuracy after self-correction:  74.71264367816092\n",
      "Incorrect Format:  13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(\n",
    "    api_key=\"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\"  # Ensure you have the Groq API Key set as an environment variable\n",
    ")\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "# Define the model for Groq API calls\n",
    "model = \"mixtral-8x7b-32768\"  # Change to the model you want to use\n",
    "\n",
    "# Prompt templates\n",
    "prompt_1 = \"Can you solve the following math problem? {problem} Explain your reasoning. Your final answer should be a single numerical number, in the form \\\\boxed{{answer}}, at the end of your response. Don't generate any additional text inside the \\\\boxed{{answer}}\"\n",
    "prompt_2 = \"Review your previous answer and find problems with your answer.\"\n",
    "prompt_3 = \"Based on the problems you found, improve your answer. Please reiterate your answer, with your final answer a single numerical number, in the form \\\\boxed{{answer}}. Don't generate any additional text inside the \\\\boxed{{answer}}\"\n",
    "\n",
    "extraction_prompt = \"Please extract the answer from the above text. Give the answer in the form \\\\boxed{{answer}} where {answer} is a single numerical number.\"\n",
    "\n",
    "# Function to make Groq API call\n",
    "def get_groq_response(chat_history):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=chat_history,\n",
    "        model=model\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "# Evaluate self-correction on a sample of the dataset\n",
    "def evaluate_self_correction(problem):\n",
    "    # Prompt 1: Solve the problem\n",
    "    \n",
    "    chat_history = [{\"role\": \"user\", \"content\": prompt_1.format(problem=problem)}]\n",
    "\n",
    "    response_1 = get_groq_response(chat_history)\n",
    "    # print(f\"Response 1: {response_1}\")\n",
    "\n",
    "    initial_answer = get_groq_response([{\"role\": \"user\", \"content\": response_1+\"\\n\"+extraction_prompt}])\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_1})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt_2})\n",
    "\n",
    "    # Prompt 2: Review the previous answer\n",
    "    response_2 = get_groq_response(chat_history)\n",
    "    # print(f\"Response 2: {response_2}\")\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response_2})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt_3})\n",
    "\n",
    "    # Prompt 3: Improve the answer based on the review\n",
    "    response_3 = get_groq_response(chat_history)\n",
    "\n",
    "    final_answer = get_groq_response([{\"role\": \"user\", \"content\": response_3+\"\\n\"+extraction_prompt}])\n",
    "    # print(f\"Response 3: {response_3}\")\n",
    "\n",
    "    return response_1, response_2, response_3, initial_answer, final_answer\n",
    "\n",
    "doubtful_responses = []\n",
    "\n",
    "def extract_correct_answer(answer_string):\n",
    "    start_index = answer_string.find(\"####\") + len(\"#### \")\n",
    "    end_index = len(answer_string)\n",
    "\n",
    "    answer = answer_string[start_index:end_index]\n",
    "\n",
    "    return int(answer)\n",
    "\n",
    "\n",
    "\n",
    "def extract_answer(response):\n",
    "\n",
    "    start_index = response.find(\"\\\\boxed{\") + len(\"\\\\boxed{\")\n",
    "\n",
    "    end_index = response.find(\"}\", start_index)\n",
    "\n",
    "    answer = response[start_index:end_index]\n",
    "\n",
    "    # if(end_index == -1):\n",
    "    #     end_index = response.find(\"]\", start_index)\n",
    "    # if(end_index == -1):\n",
    "    #     end_index = response.find(\")\", start_index)\n",
    "\n",
    "    # print(\"Response: \", response)\n",
    "\n",
    "    # print(\"Start: \", start_index)\n",
    "    # print(\"End: \", end_index)\n",
    "\n",
    "    # print(\"Value: \"+response[start_index:end_index])\n",
    "\n",
    "    # answer = \"\"\n",
    "    # flag = False\n",
    "    # for i in range(start_index, end_index):\n",
    "    #     if response[i].isdigit():\n",
    "    #         flag = True\n",
    "    #         answer += response[i]\n",
    "    #     else:\n",
    "    #         if flag:\n",
    "    #             flag=False\n",
    "    #             break\n",
    "\n",
    "    return int(answer)\n",
    "\n",
    "# Evaluate on the first 5 problems in the dataset\n",
    "\n",
    "correct_1 = 0\n",
    "correct_2 = 0\n",
    "\n",
    "for idx in range(100):\n",
    "    print(\"Idx: \", idx)\n",
    "    problem = ds[idx]['question']\n",
    "    correct_answer = extract_correct_answer(ds[idx]['answer'])\n",
    "    # print(f\"\\nProblem {idx+1}: {problem}\")\n",
    "    response_1, response_2, response_3, initial_answer, final_answer = evaluate_self_correction(problem)\n",
    "\n",
    "    # print(\"Response 1: \" + response_1)\n",
    "    # print(\"Response 2: \" + response_2)\n",
    "    # print(\"Response 3: \" + response_3)\n",
    "    # print(\"Initial Answer: \", initial_answer)\n",
    "    # print(\"Final Answer: \", final_answer)\n",
    "\n",
    "    try:\n",
    "        initial_answer = extract_answer(initial_answer)\n",
    "    except:\n",
    "        doubtful_responses.append(response_1)\n",
    "        continue\n",
    "    # try:\n",
    "    #     answer_1 = int(answer_1.split(\" \")[0])\n",
    "    # except: \n",
    "    #     print(\"Response 1: \"+ response_1)\n",
    "    #     print(\"Answer 1: \"+ answer_1)\n",
    "    #     break\n",
    "    # answer_2 = extract_answer(response_2)\n",
    "    try:\n",
    "        final_answer = extract_answer(final_answer)\n",
    "    except: \n",
    "        doubtful_responses.append(response_3)\n",
    "        continue\n",
    "    # try:\n",
    "    #     answer_2 = int(answer_2.split(\" \")[0])\n",
    "    # except: \n",
    "    #     print(\"Response 2: \"+ response_3)\n",
    "    #     print(\"Answer 2: \"+ answer_2)\n",
    "    #     break\n",
    "    # print(\"Initial Answer: \", initial_answer)\n",
    "    # print(\"Final Answer: \", final_answer)\n",
    "\n",
    "    # print(\"Initial_answer: \", initial_answer)\n",
    "    # print(\"Final_answer: \", final_answer)\n",
    "    # print(\"Correct_answer: \", correct_answer)\n",
    "    \n",
    "    if(initial_answer == correct_answer):\n",
    "        correct_1 += 1\n",
    "    if(final_answer == correct_answer):\n",
    "        correct_2 += 1\n",
    "\n",
    "\n",
    "accuracy_1 = correct_1/(100-len(doubtful_responses)) * 100\n",
    "accuracy_2 = correct_2/(100-len(doubtful_responses)) * 100\n",
    "\n",
    "print(\"GSM8K Dataset: \")\n",
    "print(\"Accuracy before self-correction: \", accuracy_1)\n",
    "print(\"Accuracy after self-correction: \", accuracy_2)\n",
    "\n",
    "print(\"Incorrect Format: \", len(doubtful_responses))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
